{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6769c4",
   "metadata": {},
   "source": [
    "# Minimax (and Alpha–Beta) - expectimax — *Othello*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea71f61",
   "metadata": {},
   "source": [
    "## Rules summary (Othello)\n",
    "\n",
    "- You must place a disc of your color on an **empty** square such that it **flanks** one or more contiguous enemy discs in at least one direction (horizontal, vertical, diagonal).  \n",
    "- All flanked enemy discs in those directions are **flipped** to your color.\n",
    "- If a player has **no legal moves**, they must **pass**; if **both** players must pass, the game ends.\n",
    "- Winner: player with **more discs** at the end (ties are possible).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f498f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Iterable, Callable, Dict\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import math, random\n",
    "from typing import Optional, Tuple, Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2146f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "BLACK, WHITE, EMPTY = 1, -1, 0\n",
    "PLAYERS = {BLACK: \"BLACK\", WHITE: \"WHITE\"}\n",
    "\n",
    "DIRS = [(-1, -1), (-1, 0), (-1, 1),\n",
    "        ( 0, -1),          ( 0, 1),\n",
    "        ( 1, -1), ( 1, 0), ( 1, 1)]\n",
    "\n",
    "def opponent(p: int) -> int:\n",
    "    return -p\n",
    "\n",
    "def on_board(r: int, c: int) -> bool:\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def initial_board() -> List[List[int]]:\n",
    "    b = [[EMPTY for _ in range(N)] for _ in range(N)]\n",
    "    b[3][3] = WHITE; b[3][4] = BLACK\n",
    "    b[4][3] = BLACK; b[4][4] = WHITE\n",
    "    return b\n",
    "\n",
    "def copy_board(b):\n",
    "    return [row[:] for row in b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c6619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flips_in_dir(board: List[List[int]], r: int, c: int, dr: int, dc: int, player: int) -> List[Tuple[int,int]]:\n",
    "    \"\"\"Return the list of opponent positions that would be flipped in one direction, else []\"\"\"\n",
    "    flips = []\n",
    "    r += dr; c += dc\n",
    "    if not on_board(r,c) or board[r][c] != opponent(player):\n",
    "        return []\n",
    "    while on_board(r,c) and board[r][c] == opponent(player):\n",
    "        flips.append((r,c))\n",
    "        r += dr; c += dc\n",
    "    if not on_board(r,c) or board[r][c] != player:\n",
    "        return []\n",
    "    return flips\n",
    "\n",
    "def legal_moves(board: List[List[int]], player: int) -> List[Optional[Tuple[int,int]]]:\n",
    "    \"\"\"Return a list of legal moves (r,c); if none exist, the only legal move is [None] (pass).\"\"\"\n",
    "    moves = []\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            if board[r][c] != EMPTY:\n",
    "                continue\n",
    "            flips_any = []\n",
    "            for dr,dc in DIRS:\n",
    "                flips_any += _flips_in_dir(board, r, c, dr, dc, player)\n",
    "            if flips_any:\n",
    "                moves.append((r,c))\n",
    "    if not moves:\n",
    "        return [None]\n",
    "    return moves\n",
    "\n",
    "def apply_move(board: List[List[int]], move: Optional[Tuple[int,int]], player: int) -> List[List[int]]:\n",
    "    \"\"\"Return a NEW board after applying move (or pass=None).\"\"\"\n",
    "    if move is None:\n",
    "        return copy_board(board)\n",
    "    r, c = move\n",
    "    assert board[r][c] == EMPTY, \"illegal: target not empty\"\n",
    "    flips_all = []\n",
    "    for dr,dc in DIRS:\n",
    "        flips_all += _flips_in_dir(board, r, c, dr, dc, player)\n",
    "    assert flips_all, \"illegal: no flips created\"\n",
    "    nb = copy_board(board)\n",
    "    nb[r][c] = player\n",
    "    for fr,fc in flips_all:\n",
    "        nb[fr][fc] = player\n",
    "    return nb\n",
    "\n",
    "def count_discs(board: List[List[int]]) -> Tuple[int,int,int]:\n",
    "    b = sum(cell == BLACK for row in board for cell in row)\n",
    "    w = sum(cell == WHITE for row in board for cell in row)\n",
    "    e = N*N - b - w\n",
    "    return b, w, e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33871e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flips_in_dir(board: List[List[int]], r: int, c: int, dr: int, dc: int, player: int) -> List[Tuple[int,int]]:\n",
    "    \"\"\"Return the list of opponent positions that would be flipped in one direction, else []\"\"\"\n",
    "    flips = []\n",
    "    r += dr; c += dc\n",
    "    if not on_board(r,c) or board[r][c] != opponent(player):\n",
    "        return []\n",
    "    while on_board(r,c) and board[r][c] == opponent(player):\n",
    "        flips.append((r,c))\n",
    "        r += dr; c += dc\n",
    "    if not on_board(r,c) or board[r][c] != player:\n",
    "        return []\n",
    "    return flips\n",
    "\n",
    "def legal_moves(board: List[List[int]], player: int) -> List[Optional[Tuple[int,int]]]:\n",
    "    \"\"\"Return a list of legal moves (r,c); if none exist, the only legal move is [None] (pass).\"\"\"\n",
    "    moves = []\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            if board[r][c] != EMPTY:\n",
    "                continue\n",
    "            flips_any = []\n",
    "            for dr,dc in DIRS:\n",
    "                flips_any += _flips_in_dir(board, r, c, dr, dc, player)\n",
    "            if flips_any:\n",
    "                moves.append((r,c))\n",
    "    if not moves:\n",
    "        return [None]\n",
    "    return moves\n",
    "\n",
    "def apply_move(board: List[List[int]], move: Optional[Tuple[int,int]], player: int) -> List[List[int]]:\n",
    "    \"\"\"Return a NEW board after applying move (or pass=None).\"\"\"\n",
    "    if move is None:\n",
    "        return copy_board(board)\n",
    "    r, c = move\n",
    "    assert board[r][c] == EMPTY, \"illegal: target not empty\"\n",
    "    flips_all = []\n",
    "    for dr,dc in DIRS:\n",
    "        flips_all += _flips_in_dir(board, r, c, dr, dc, player)\n",
    "    assert flips_all, \"illegal: no flips created\"\n",
    "    nb = copy_board(board)\n",
    "    nb[r][c] = player\n",
    "    for fr,fc in flips_all:\n",
    "        nb[fr][fc] = player\n",
    "    return nb\n",
    "\n",
    "def count_discs(board: List[List[int]]) -> Tuple[int,int,int]:\n",
    "    b = sum(cell == BLACK for row in board for cell in row)\n",
    "    w = sum(cell == WHITE for row in board for cell in row)\n",
    "    e = N*N - b - w\n",
    "    return b, w, e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "210c470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(board: List[List[int]]):\n",
    "    header = \"    \" + \" \".join(str(c) for c in range(N))\n",
    "    print(header)\n",
    "    print(\"   \" + \"--\"*N)\n",
    "    for r in range(N):\n",
    "        row = []\n",
    "        for c in range(N):\n",
    "            v = board[r][c]\n",
    "            row.append(\"●\" if v == BLACK else \"○\" if v == WHITE else \".\")\n",
    "        print(f\"{r:>2} | \" + \" \".join(row))\n",
    "    b,w,e = count_discs(board)\n",
    "    print(f\"    (●=BLACK {b}, ○=WHITE {w}, empty {e})\")\n",
    "\n",
    "def pretty_move(m):\n",
    "    return \"pass\" if m is None else f\"({m[0]},{m[1]})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846f8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    name = \"Agent\"\n",
    "    def action(self, state: OthelloState) -> Optional[Tuple[int,int]]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    name = \"Random\"\n",
    "    def action(self, state: OthelloState):\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "        return random.choice(moves)\n",
    "\n",
    "class GreedyFlipAgent(Agent):\n",
    "    \"\"\"Pick the move that flips the most discs immediately; pass if only pass.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"GreedyFlips\"\n",
    "    def action(self, state: OthelloState):\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "        if moves == [None]:\n",
    "            return None\n",
    "        best = None\n",
    "        best_flips = -1\n",
    "        for m in moves:\n",
    "            r,c = m\n",
    "            flips = 0\n",
    "            for dr,dc in DIRS:\n",
    "                flips += len(_flips_in_dir(b, r, c, dr, dc, state.player))\n",
    "            if flips > best_flips:\n",
    "                best_flips, best = flips, m\n",
    "        return best\n",
    "\n",
    "def play_game(black_agent: Agent, white_agent: Agent, seed: Optional[int] = None, verbose: bool = False):\n",
    "    rng = random.Random(seed)\n",
    "    random.seed(seed)\n",
    "    board = initial_board()\n",
    "    player = BLACK\n",
    "    state = OthelloState.from_board(board, player)\n",
    "    history = []\n",
    "    if verbose:\n",
    "        print_board(board)\n",
    "    while not is_terminal(state):\n",
    "        agent = black_agent if state.player == BLACK else white_agent\n",
    "        move = agent.action(state)\n",
    "        moves = legal_moves(state.to_board(), state.player)\n",
    "        assert move in moves, f\"{agent.name} chose illegal move {move}; legal are {moves}\"\n",
    "        if move is None:\n",
    "            history.append((state.player, None))\n",
    "            state = OthelloState.from_board(state.to_board(), opponent(state.player))\n",
    "            if verbose:\n",
    "                print(f\"{PLAYERS[opponent(state.player)]} passes.\")\n",
    "            continue\n",
    "        new_board = apply_move(state.to_board(), move, state.player)\n",
    "        history.append((state.player, move))\n",
    "        state = OthelloState.from_board(new_board, opponent(state.player))\n",
    "        if verbose:\n",
    "            print(f\"{PLAYERS[opponent(state.player)]} played {pretty_move(move)}\")\n",
    "            print_board(new_board)\n",
    "\n",
    "    b,w,e = count_discs(state.to_board())\n",
    "    result = 1 if b > w else -1 if w > b else 0\n",
    "    if verbose:\n",
    "        print(\"Final:\")\n",
    "        print_board(state.to_board())\n",
    "        if result == 0:\n",
    "            print(\"Draw.\")\n",
    "        else:\n",
    "            print(f\"Winner: {PLAYERS[result]}\")\n",
    "    return result, history  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cec58",
   "metadata": {},
   "source": [
    "## Evaluation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a3e1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORNER_CELLS = [(0,0), (0,7), (7,0), (7,7)]\n",
    "\n",
    "def dynamic_weight(total_discs: int) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Returns a dynamically adjusted weight matrix based on the current game phase.\n",
    "    \"\"\"\n",
    "    if total_discs < 20:\n",
    "        return [\n",
    "            [ 4, 3,  2,  2,  2,  2,  3,  4],\n",
    "            [ 3, 2,  1,  1,  1,  1,  2,  3],\n",
    "            [ 2, 1,  1,  0,  0,  1,  1,  2],\n",
    "            [ 2, 1,  0,  1,  1,  0,  1,  2],\n",
    "            [ 2, 1,  0,  1,  1,  0,  1,  2],\n",
    "            [ 2, 1,  1,  0,  0,  1,  1,  2],\n",
    "            [ 3, 2,  1,  1,  1,  1,  2,  3],\n",
    "            [ 4, 3,  2,  2,  2,  2,  3,  4]\n",
    "        ]\n",
    "    elif total_discs < 50:\n",
    "        return [\n",
    "            [ 3, 2,  1,  1,  1,  1,  2,  3],\n",
    "            [ 2, 2,  1,  1,  1,  1,  2,  2],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 2, 2,  1,  1,  1,  1,  2,  2],\n",
    "            [ 3, 2,  1,  1,  1,  1,  2,  3]\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1, 1,  1,  1,  1,  1,  1,  1]\n",
    "        ]\n",
    "        \n",
    "def evaluate(board: List[List[int]], perspective: int) -> float:\n",
    "    \"\"\"\n",
    "    Returns a scalar value representing the evaluation of the board from the\n",
    "    perspective of the current player.\n",
    "    \"\"\"\n",
    "    b, w, e = count_discs(board)\n",
    "    \n",
    "    discs_score = b / (b + w) if perspective == BLACK else w / (b + w)\n",
    "    \n",
    "    my_moves = len(legal_moves(board, perspective))\n",
    "    opp_moves = len(legal_moves(board, opponent(perspective)))\n",
    "    move_score = my_moves / (my_moves + opp_moves) if (my_moves + opp_moves) > 0 else 0\n",
    "\n",
    "    \n",
    "    corner_score = 0\n",
    "    for (r, c) in CORNER_CELLS:\n",
    "        if board[r][c] == perspective:\n",
    "            corner_score += 0.25\n",
    "    \n",
    "    weight_matrix = dynamic_weight(b + w)\n",
    "    edge_score = 0\n",
    "    for r in range(8):\n",
    "        for c in range(8):\n",
    "            if board[r][c] == perspective:\n",
    "                edge_score += weight_matrix[r][c]\n",
    "    \n",
    "    final_score = 0.5 * discs_score + 0.2 * move_score + 0.15 * corner_score + 0.15 * edge_score\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a610192",
   "metadata": {},
   "source": [
    "## Implement **Minimax**\n",
    "\n",
    "Implement a `MinimaxAgent` that:\n",
    "- Searches to a fixed **depth**.\n",
    "- Uses `evaluate(board, perspective)` at depth cutoffs (non-terminal leaves).\n",
    "- Treats the current player as the **maximizer** and the opponent as **minimizer**.\n",
    "- Returns the **best legal move** for `state.player`.\n",
    "\n",
    "Note: use self.nodes and count the number of nodes visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ec52fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxAgent(Agent):\n",
    "    def __init__(self, depth: int = 4, eval_fn: Callable[[List[List[int]], int], float] = evaluate):\n",
    "        self.depth = depth\n",
    "        self.eval_fn = eval_fn\n",
    "        self.name = f\"Minimax(d={depth})\"\n",
    "        self.nodes = 0  \n",
    "\n",
    "    def action(self, state: OthelloState) -> Optional[Tuple[int, int]]:\n",
    "        perspective = state.player\n",
    "        board = state.to_board()\n",
    "        moves = legal_moves(board, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            return None\n",
    "\n",
    "        best_move = None\n",
    "        best_val = -math.inf\n",
    "\n",
    "        for move in moves:\n",
    "            next_board = apply_move(board, move, state.player)\n",
    "            next_state = OthelloState(next_board, opponent(state.player))\n",
    "            val = self._min_value(next_state, self.depth - 1, perspective)\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_move = move\n",
    "                \n",
    "        print(f\"{self.name} explored {self.nodes} nodes.\")\n",
    "        return best_move\n",
    "\n",
    "\n",
    "    def _max_value(self, state: OthelloState, depth: int, perspective: int) -> float:\n",
    "        self.nodes += 1\n",
    "        if is_terminal(state):\n",
    "            return utility(state, perspective)\n",
    "\n",
    "        if depth <= 0:\n",
    "            self.nodes += 1\n",
    "            return self.eval_fn(state.to_board(), perspective)\n",
    "\n",
    "        board = state.to_board()\n",
    "        moves = legal_moves(board, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            next_board = apply_move(board, None, state.player)\n",
    "            next_state = OthelloState(next_board, opponent(state.player))\n",
    "            return self._min_value(next_state, depth - 1, perspective)\n",
    "\n",
    "        value = -math.inf\n",
    "\n",
    "        for move in moves:\n",
    "            next_board = apply_move(board, move, state.player)\n",
    "            next_state = OthelloState(next_board, opponent(state.player))\n",
    "            val = self._min_value(next_state, depth - 1, perspective)\n",
    "            value = max(value, val)\n",
    "        return value\n",
    "\n",
    "\n",
    "    def _min_value(self, state: OthelloState, depth: int, perspective: int) -> float:\n",
    "        self.nodes += 1\n",
    "        if is_terminal(state):\n",
    "            return utility(state, perspective)\n",
    "\n",
    "        if depth <= 0:\n",
    "            return self.eval_fn(state.to_board(), perspective)\n",
    "\n",
    "        board = state.to_board()\n",
    "        moves = legal_moves(board, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            next_board = apply_move(board, None, state.player)\n",
    "            next_state = OthelloState(next_board, opponent(state.player))\n",
    "            return self._max_value(next_state, depth - 1, perspective)\n",
    "\n",
    "        value = math.inf\n",
    "\n",
    "        for move in moves:\n",
    "            next_board = apply_move(board, move, state.player)\n",
    "            next_state = OthelloState(next_board, opponent(state.player))\n",
    "            value_temp = self._max_value(next_state, depth - 1, perspective)\n",
    "            value = min(value, value_temp)\n",
    "\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549db61",
   "metadata": {},
   "source": [
    "## **Alpha–Beta pruning**\n",
    "\n",
    "Implement `AlphaBetaAgent` with the same interface but pruned search:\n",
    "- Maintain `alpha` (best value for max so far) and `beta` (best for min).\n",
    "- Prune when `value >= beta` at max nodes, or `value <= alpha` at min nodes.\n",
    "- Keep the same evaluation function and depth semantics.\n",
    "\n",
    "Note: use self.nodes and count the number of nodes visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a56d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBetaAgent(Agent):\n",
    "    def __init__(self, depth: int = 5, eval_fn: Callable[[List[List[int]], int], float] = evaluate):\n",
    "        self.depth = depth\n",
    "        self.eval_fn = eval_fn\n",
    "        self.name = f\"AlphaBeta(d={depth})\"\n",
    "        self.nodes = 0\n",
    "\n",
    "    def action(self, state: OthelloState) -> Optional[Tuple[int, int]]:\n",
    "        perspective = state.player\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            return None\n",
    "\n",
    "        def order_key(m):\n",
    "            next_board = apply_move(b, m, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self.eval_fn(next_state.to_board(), perspective)\n",
    "\n",
    "        moves.sort(key=order_key, reverse=True)\n",
    "\n",
    "        alpha, beta = -math.inf, math.inf\n",
    "        best_move, best_val = None, -math.inf\n",
    "\n",
    "        for move in moves:\n",
    "            next_board = apply_move(b, move, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            val = self._min_value(next_state, self.depth - 1, perspective, alpha, beta)\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_move = move\n",
    "            alpha = max(alpha, best_val)\n",
    "            if beta <= alpha:\n",
    "                break  \n",
    "        \n",
    "        print(f\"{self.name} explored {self.nodes} nodes.\")\n",
    "        return best_move\n",
    "\n",
    "\n",
    "    def _max_value(self, state: OthelloState, depth: int, perspective: int, alpha: float, beta: float) -> float:\n",
    "        self.nodes += 1\n",
    "        if is_terminal(state):\n",
    "            return utility(state, perspective)\n",
    "\n",
    "        if depth <= 0:\n",
    "            return self.eval_fn(state.to_board(), perspective)\n",
    "\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            next_board = apply_move(b, None, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self._min_value(next_state, depth - 1, perspective, alpha, beta)\n",
    "\n",
    "        def order_key(m):\n",
    "            next_board = apply_move(b, m, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self.eval_fn(next_state.to_board(), perspective)\n",
    "\n",
    "        moves.sort(key=order_key, reverse=True)\n",
    "\n",
    "        value = -math.inf\n",
    "        for move in moves:\n",
    "            next_board = apply_move(b, move, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            val = self._min_value(next_state, depth - 1, perspective, alpha, beta)\n",
    "            value = max(value, val)\n",
    "            alpha = max(alpha, value)\n",
    "            if beta <= alpha:\n",
    "                break  \n",
    "        return value\n",
    "\n",
    "\n",
    "    def _min_value(self, state: OthelloState, depth: int, perspective: int, alpha: float, beta: float) -> float:\n",
    "        self.nodes += 1\n",
    "        if is_terminal(state):\n",
    "            return utility(state, perspective)\n",
    "\n",
    "        if depth <= 0:\n",
    "            return self.eval_fn(state.to_board(), perspective)\n",
    "\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            next_board = apply_move(b, None, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self._max_value(next_state, depth - 1, perspective, alpha, beta)\n",
    "\n",
    "        def order_key(m):\n",
    "            next_board = apply_move(b, m, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self.eval_fn(next_state.to_board(), perspective)\n",
    "\n",
    "        moves.sort(key=order_key, reverse=True)\n",
    "\n",
    "        value = math.inf\n",
    "        for move in moves:\n",
    "            next_board = apply_move(b, move, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            val = self._max_value(next_state, depth - 1, perspective, alpha, beta)\n",
    "            value = min(value, val)\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break  \n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fbdf54",
   "metadata": {},
   "source": [
    "## Expectimax with Imperfect Opponent (Risk-aware)\n",
    "\n",
    "In this part you’ll implement **Expectimax** where the **opponent is modeled as a chance node**:\n",
    "\n",
    "- With probability **(1 − ε)** the opponent plays the **best** move for them (i.e., the move that **minimizes our value**).\n",
    "- With probability **ε** the opponent plays **a suboptimal move**, chosen **uniformly among the remaining legal moves**.\n",
    "\n",
    "This allows our agent to sometimes choose a move that is *risky* under strict minimax, but has **higher expected value** if the opponent occasionally blunders.\n",
    "\n",
    "Note: use self.nodes and count the number of nodes visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed313de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, Callable, List\n",
    "\n",
    "class ExpectimaxRiskyAgent(Agent):\n",
    "    \"\"\"\n",
    "    Expectimax with an *imperfect opponent*:\n",
    "      - With prob (1 - eps), opponent plays the minimizing move (best for them).\n",
    "      - With prob eps, opponent plays uniformly among the *other* legal moves.\n",
    "\n",
    "    depth:        search depth in plies\n",
    "    eval_fn:      evaluation(board, perspective) -> float   (use your own evaluate)\n",
    "    blunder_rate: epsilon in [0,1]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 depth: int = 4,\n",
    "                 eval_fn: Callable[[List[List[int]], int], float] = evaluate,\n",
    "                 blunder_rate: float = 0.20):\n",
    "        self.depth = depth\n",
    "        self.eval_fn = eval_fn\n",
    "        self.eps = max(0.0, min(1.0, blunder_rate))\n",
    "        self.name = f\"ExpectimaxRisky(d={depth}, eps={self.eps:.2f})\"\n",
    "        self.nodes = 0\n",
    "\n",
    "    def action(self, state: OthelloState) -> Optional[Tuple[int,int]]:\n",
    "        \"\"\"\n",
    "        TODO: Return the legal move that maximizes our *expected* value under the imperfect-opponent model.\n",
    "        - If no legal move (only [None]), return None (pass).\n",
    "        - For each legal move m:\n",
    "            compute expected value after m by calling _opponent_expectation on the child state.\n",
    "        - Pick the argmax by value.\n",
    "        \"\"\"\n",
    "        board = state.to_board()\n",
    "        moves = legal_moves(board, state.player)\n",
    "        if moves == [None]:\n",
    "            return None\n",
    "\n",
    "        perspective = state.player\n",
    "        best_move, best_val = None, -math.inf\n",
    "        for move in moves:\n",
    "            next_board = apply_move(board, move, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            val = self._opponent_expectation(next_state, self.depth - 1, perspective)\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_move = move\n",
    "\n",
    "        print(f\"{self.name} explored {self.nodes} nodes.\")\n",
    "        return best_move\n",
    "\n",
    "    def _max_value(self, state: OthelloState, depth: int, perspective: int) -> float:\n",
    "        self.nodes += 1\n",
    "        if is_terminal(state):\n",
    "            return utility(state, perspective)\n",
    "\n",
    "        if depth <= 0:\n",
    "            return self.eval_fn(state.to_board(), perspective)\n",
    "\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            next_board = apply_move(b, None, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self._opponent_expectation(next_state, depth - 1, perspective)\n",
    "\n",
    "        value = -math.inf\n",
    "        for move in moves:\n",
    "            next_board = apply_move(b, move, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            val = self._opponent_expectation(next_state, depth - 1, perspective)\n",
    "            value = max(value, val)\n",
    "        return value\n",
    "\n",
    "    def _opponent_expectation(self, state: OthelloState, depth: int, perspective: int) -> float:\n",
    "        self.nodes += 1\n",
    "        if is_terminal(state):\n",
    "            return utility(state, perspective)\n",
    "\n",
    "        if depth <= 0:\n",
    "            return self.eval_fn(state.to_board(), perspective)\n",
    "\n",
    "        b = state.to_board()\n",
    "        moves = legal_moves(b, state.player)\n",
    "\n",
    "        if moves == [None]:\n",
    "            next_board = apply_move(b, None, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            return self._max_value(next_state, depth - 1, perspective)\n",
    "\n",
    "        vals = []\n",
    "        for m in moves:\n",
    "            next_board = apply_move(b, m, state.player)\n",
    "            next_state = OthelloState.from_board(next_board, opponent(state.player))\n",
    "            v = self._max_value(next_state, depth - 1, perspective)\n",
    "            vals.append((m, v))\n",
    "\n",
    "        vals.sort(key=lambda x: x[1])\n",
    "        best_move, best_val = vals[0]\n",
    "        others = vals[1:]\n",
    "\n",
    "        if len(vals) == 1 or self.eps <= 1e-12:\n",
    "            return best_val\n",
    "\n",
    "        eps = self.eps\n",
    "        avg_others = sum(v for (_, v) in others) / len(others)\n",
    "        exp_val = (1 - eps) * best_val + eps * avg_others\n",
    "        return exp_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e4087",
   "metadata": {},
   "source": [
    "## engine checks\n",
    "\n",
    "Run these to verify rules implementation (not search).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6bc209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . . . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 2, ○=WHITE 2, empty 60)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 1, empty 59)\n",
      "✓ Basic engine checks passed.\n"
     ]
    }
   ],
   "source": [
    "b = initial_board()\n",
    "print_board(b)\n",
    "\n",
    "lm_black = legal_moves(b, BLACK)\n",
    "lm_white = legal_moves(b, WHITE)\n",
    "assert len([m for m in lm_black if m is not None]) == 4\n",
    "assert len([m for m in lm_white if m is not None]) == 4\n",
    "\n",
    "assert (2,3) in lm_black\n",
    "b2 = apply_move(b, (2,3), BLACK)\n",
    "print_board(b2)\n",
    "\n",
    "print(\"✓ Basic engine checks passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e762d",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "Play a few games between baseline agents and your search agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b56c1ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . . . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 2, ○=WHITE 2, empty 60)\n",
      "Minimax(d=3) explored 72 nodes.\n",
      "BLACK played (2,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 1, empty 59)\n",
      "AlphaBeta(d=3) explored 51 nodes.\n",
      "WHITE played (4,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 3, empty 58)\n",
      "Minimax(d=3) explored 220 nodes.\n",
      "BLACK played (5,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 5, ○=WHITE 2, empty 57)\n",
      "AlphaBeta(d=3) explored 138 nodes.\n",
      "WHITE played (1,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 5, empty 56)\n",
      "Minimax(d=3) explored 424 nodes.\n",
      "BLACK played (4,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 3, empty 55)\n",
      "AlphaBeta(d=3) explored 258 nodes.\n",
      "WHITE played (6,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 6, empty 54)\n",
      "Minimax(d=3) explored 670 nodes.\n",
      "BLACK played (4,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● ● . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 5, empty 53)\n",
      "AlphaBeta(d=3) explored 318 nodes.\n",
      "WHITE played (5,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ○ . . .\n",
      " 4 | . ● ● ● ● ○ . .\n",
      " 5 | . . . . . ○ ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 8, empty 52)\n",
      "Minimax(d=3) explored 997 nodes.\n",
      "BLACK played (0,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | . ● ● ● ● ○ . .\n",
      " 5 | . . . . . ○ ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 8, ○=WHITE 5, empty 51)\n",
      "AlphaBeta(d=3) explored 428 nodes.\n",
      "WHITE played (4,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ○ ○ . .\n",
      " 5 | . . . . . ○ ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 10, empty 50)\n",
      "Minimax(d=3) explored 1225 nodes.\n",
      "BLACK played (7,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 7, empty 49)\n",
      "AlphaBeta(d=3) explored 545 nodes.\n",
      "WHITE played (7,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 7, ○=WHITE 9, empty 48)\n",
      "Minimax(d=3) explored 1707 nodes.\n",
      "BLACK played (5,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 9, ○=WHITE 8, empty 47)\n",
      "AlphaBeta(d=3) explored 831 nodes.\n",
      "WHITE played (6,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ ●\n",
      " 6 | . . . . . . ○ ○\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 8, ○=WHITE 10, empty 46)\n",
      "Minimax(d=3) explored 2062 nodes.\n",
      "BLACK played (7,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ ●\n",
      " 6 | . . . . . . ● ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 11, ○=WHITE 8, empty 45)\n",
      "AlphaBeta(d=3) explored 1005 nodes.\n",
      "WHITE played (6,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ○ ○ ●\n",
      " 6 | . . . . . ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 9, ○=WHITE 11, empty 44)\n",
      "Minimax(d=3) explored 2450 nodes.\n",
      "BLACK played (3,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● . .\n",
      " 5 | . . . . . ● ○ ●\n",
      " 6 | . . . . . ● ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 14, ○=WHITE 7, empty 43)\n",
      "AlphaBeta(d=3) explored 1250 nodes.\n",
      "WHITE played (1,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ● ○ ● . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ ●\n",
      " 6 | . . . . . ● ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 11, ○=WHITE 11, empty 42)\n",
      "Minimax(d=3) explored 3060 nodes.\n",
      "BLACK played (4,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ● ○ ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . . ● ● ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 15, ○=WHITE 8, empty 41)\n",
      "AlphaBeta(d=3) explored 1407 nodes.\n",
      "WHITE played (6,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ● ○ ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 13, ○=WHITE 11, empty 40)\n",
      "Minimax(d=3) explored 3840 nodes.\n",
      "BLACK played (0,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 8, empty 39)\n",
      "AlphaBeta(d=3) explored 1677 nodes.\n",
      "WHITE played (2,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 16, ○=WHITE 10, empty 38)\n",
      "Minimax(d=3) explored 4523 nodes.\n",
      "BLACK played (7,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 9, empty 37)\n",
      "AlphaBeta(d=3) explored 1973 nodes.\n",
      "WHITE played (0,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● ○ ● . . . .\n",
      " 1 | . . ● ○ . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 11, empty 36)\n",
      "Minimax(d=3) explored 5366 nodes.\n",
      "BLACK played (1,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 9, empty 35)\n",
      "AlphaBeta(d=3) explored 2213 nodes.\n",
      "WHITE played (0,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 11, empty 34)\n",
      "Minimax(d=3) explored 5904 nodes.\n",
      "BLACK played (5,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 8, empty 33)\n",
      "AlphaBeta(d=3) explored 2405 nodes.\n",
      "WHITE played (0,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 10, empty 32)\n",
      "Minimax(d=3) explored 5993 nodes.\n",
      "BLACK played (3,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . ● . ● ● ● . .\n",
      " 4 | ○ ○ ● ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 24, ○=WHITE 9, empty 31)\n",
      "AlphaBeta(d=3) explored 2549 nodes.\n",
      "WHITE played (6,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● . . .\n",
      " 2 | . . . ○ ● . . .\n",
      " 3 | . ● . ○ ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 17, empty 30)\n",
      "Minimax(d=3) explored 6771 nodes.\n",
      "BLACK played (3,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . ● ● ● ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 15, empty 29)\n",
      "AlphaBeta(d=3) explored 3094 nodes.\n",
      "WHITE played (0,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ● ○ ○ . . .\n",
      " 2 | . . . ○ ● . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 19, empty 28)\n",
      "Minimax(d=3) explored 7502 nodes.\n",
      "BLACK played (2,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ● ○ ○ . . .\n",
      " 2 | . . ● ● ● . . .\n",
      " 3 | . ● ● ● ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 17, empty 27)\n",
      "AlphaBeta(d=3) explored 3464 nodes.\n",
      "WHITE played (5,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ○ ○ ○ . . .\n",
      " 2 | . . ○ ● ● . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . ○ ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 16, ○=WHITE 22, empty 26)\n",
      "Minimax(d=3) explored 8450 nodes.\n",
      "BLACK played (7,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ○ ○ ○ . . .\n",
      " 2 | . . ○ ● ● . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 18, empty 25)\n",
      "AlphaBeta(d=3) explored 3886 nodes.\n",
      "WHITE played (1,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ○ ○ ○ ○ . .\n",
      " 2 | . . ○ ● ○ . . .\n",
      " 3 | . ● ○ ○ ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 21, empty 24)\n",
      "Minimax(d=3) explored 9046 nodes.\n",
      "BLACK played (1,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . ● ○ ○ ○ ○ . .\n",
      " 2 | . . ● ● ○ . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 19, empty 23)\n",
      "AlphaBeta(d=3) explored 4196 nodes.\n",
      "WHITE played (2,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . ○ ○ ○ ○ ○ . .\n",
      " 2 | . ○ ○ ○ ○ . . .\n",
      " 3 | . ○ ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 24, empty 22)\n",
      "Minimax(d=3) explored 9452 nodes.\n",
      "BLACK played (1,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ● ○ ○ ○ ○ ○ . .\n",
      " 2 | . ● ○ ○ ○ . . .\n",
      " 3 | . ○ ● ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 22, empty 21)\n",
      "AlphaBeta(d=3) explored 4519 nodes.\n",
      "WHITE played (2,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ . . .\n",
      " 3 | . ○ ● ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 25, empty 20)\n",
      "Minimax(d=3) explored 9794 nodes.\n",
      "BLACK played (3,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ . . .\n",
      " 3 | ● ● ● ● ● ● . .\n",
      " 4 | ○ ● ○ ● ● ● ● .\n",
      " 5 | . . ● ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 22, empty 19)\n",
      "AlphaBeta(d=3) explored 4656 nodes.\n",
      "WHITE played (5,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ . . .\n",
      " 3 | ● ● ○ ● ○ ● . .\n",
      " 4 | ○ ● ○ ○ ○ ● ● .\n",
      " 5 | . . ● ● ○ ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 27, empty 18)\n",
      "Minimax(d=3) explored 10000 nodes.\n",
      "BLACK played (0,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 1 | ○ ○ ○ ○ ○ ● . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | ● ● ○ ● ○ ● . .\n",
      " 4 | ○ ● ○ ○ ○ ● ● .\n",
      " 5 | . . ● ● ○ ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 25, empty 17)\n",
      "AlphaBeta(d=3) explored 4900 nodes.\n",
      "WHITE played (2,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ○ . .\n",
      " 4 | ○ ● ○ ○ ○ ○ ● .\n",
      " 5 | . . ● ● ○ ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 31, empty 16)\n",
      "Minimax(d=3) explored 10272 nodes.\n",
      "BLACK played (1,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 1 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 2 | ○ ○ ○ ○ ○ ● . .\n",
      " 3 | ● ● ○ ● ● ○ . .\n",
      " 4 | ○ ● ○ ● ○ ○ ● .\n",
      " 5 | . . ● ● ○ ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 28, empty 15)\n",
      "AlphaBeta(d=3) explored 5180 nodes.\n",
      "WHITE played (0,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 2 | ○ ○ ○ ○ ○ ● . .\n",
      " 3 | ● ● ○ ● ● ○ . .\n",
      " 4 | ○ ● ○ ● ○ ○ ● .\n",
      " 5 | . . ● ● ○ ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 30, empty 14)\n",
      "Minimax(d=3) explored 10475 nodes.\n",
      "BLACK played (3,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 2 | ○ ○ ○ ○ ○ ● . .\n",
      " 3 | ● ● ○ ● ● ● ● .\n",
      " 4 | ○ ● ○ ● ○ ● ● .\n",
      " 5 | . . ● ● ● ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 24, ○=WHITE 27, empty 13)\n",
      "AlphaBeta(d=3) explored 5365 nodes.\n",
      "WHITE played (6,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ● ○ ○ ○ ● ● .\n",
      " 5 | . . ○ ● ● ○ ● ●\n",
      " 6 | . ○ . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 33, empty 12)\n",
      "Minimax(d=3) explored 10541 nodes.\n",
      "BLACK played (5,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ● ● ○ ○ ● ● .\n",
      " 5 | . ● ● ● ● ○ ● ●\n",
      " 6 | . ○ . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 31, empty 11)\n",
      "AlphaBeta(d=3) explored 5509 nodes.\n",
      "WHITE played (5,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ○ ● ○ ○ ● ● .\n",
      " 5 | ○ ○ ○ ○ ○ ○ ● ●\n",
      " 6 | . ○ . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 37, empty 10)\n",
      "Minimax(d=3) explored 10646 nodes.\n",
      "BLACK played (6,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 34, empty 9)\n",
      "AlphaBeta(d=3) explored 5596 nodes.\n",
      "WHITE played (3,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ○ ○ ○\n",
      " 4 | ○ ○ ● ○ ● ● ○ .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 38, empty 8)\n",
      "Minimax(d=3) explored 10764 nodes.\n",
      "BLACK played (2,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 3 | ● ● ○ ● ○ ● ● ○\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 35, empty 7)\n",
      "AlphaBeta(d=3) explored 5642 nodes.\n",
      "WHITE played (2,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 39, empty 6)\n",
      "Minimax(d=3) explored 10814 nodes.\n",
      "BLACK played (1,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 38, empty 5)\n",
      "AlphaBeta(d=3) explored 5660 nodes.\n",
      "WHITE played (4,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 5 | ○ ○ ● ● ○ ○ ○ ○\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 42, empty 4)\n",
      "Minimax(d=3) explored 10840 nodes.\n",
      "BLACK played (7,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 5 | ○ ○ ● ● ○ ○ ○ ○\n",
      " 6 | . ● ● ● ● ○ ○ ○\n",
      " 7 | ● . . ● ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 41, empty 3)\n",
      "AlphaBeta(d=3) explored 5675 nodes.\n",
      "WHITE played (7,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ○ ○ ● ○ ○ ○\n",
      " 5 | ○ ○ ○ ● ○ ○ ○ ○\n",
      " 6 | . ○ ○ ○ ● ○ ○ ○\n",
      " 7 | ● . ○ ● ● ● ● ●\n",
      "    (●=BLACK 15, ○=WHITE 47, empty 2)\n",
      "Minimax(d=3) explored 10844 nodes.\n",
      "BLACK played (6,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ● ○ ● ○ ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ○ ○ ○ ○\n",
      " 6 | ● ● ● ● ● ○ ○ ○\n",
      " 7 | ● . ○ ● ● ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 40, empty 1)\n",
      "AlphaBeta(d=3) explored 5676 nodes.\n",
      "WHITE played (7,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ● ○ ● ○ ● ○ ○ ○\n",
      " 5 | ● ○ ○ ● ○ ○ ○ ○\n",
      " 6 | ● ○ ● ● ● ○ ○ ○\n",
      " 7 | ● ○ ○ ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 43, empty 0)\n",
      "Final:\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ● ○ ● ○ ● ○ ○ ○\n",
      " 5 | ● ○ ○ ● ○ ○ ○ ○\n",
      " 6 | ● ○ ● ● ● ○ ○ ○\n",
      " 7 | ● ○ ○ ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 43, empty 0)\n",
      "Winner: WHITE\n",
      "Minimax vs AlphaBeta (Depth 3): -1 | nodes: 10844 / 5676\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . . . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 2, ○=WHITE 2, empty 60)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 72 nodes.\n",
      "BLACK played (2,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 1, empty 59)\n",
      "Minimax(d=3) explored 10922 nodes.\n",
      "WHITE played (4,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 3, empty 58)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 220 nodes.\n",
      "BLACK played (5,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 5, ○=WHITE 2, empty 57)\n",
      "Minimax(d=3) explored 11085 nodes.\n",
      "WHITE played (1,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 5, empty 56)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 424 nodes.\n",
      "BLACK played (4,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 3, empty 55)\n",
      "Minimax(d=3) explored 11288 nodes.\n",
      "WHITE played (6,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 6, empty 54)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 670 nodes.\n",
      "BLACK played (4,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● ● . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 5, empty 53)\n",
      "Minimax(d=3) explored 11516 nodes.\n",
      "WHITE played (5,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ○ . . .\n",
      " 4 | . ● ● ● ● ○ . .\n",
      " 5 | . . . . . ○ ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 8, empty 52)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 997 nodes.\n",
      "BLACK played (0,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | . ● ● ● ● ○ . .\n",
      " 5 | . . . . . ○ ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 8, ○=WHITE 5, empty 51)\n",
      "Minimax(d=3) explored 11836 nodes.\n",
      "WHITE played (4,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ○ ○ . .\n",
      " 5 | . . . . . ○ ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 10, empty 50)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 1225 nodes.\n",
      "BLACK played (7,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 7, empty 49)\n",
      "Minimax(d=3) explored 12185 nodes.\n",
      "WHITE played (7,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 7, ○=WHITE 9, empty 48)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 1707 nodes.\n",
      "BLACK played (5,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 9, ○=WHITE 8, empty 47)\n",
      "Minimax(d=3) explored 12657 nodes.\n",
      "WHITE played (6,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ○ ● ●\n",
      " 6 | . . . . . ○ ○ .\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 8, ○=WHITE 10, empty 46)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 2366 nodes.\n",
      "BLACK played (7,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ○ ● ●\n",
      " 6 | . . . . . ○ ● .\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 11, ○=WHITE 8, empty 45)\n",
      "Minimax(d=3) explored 13119 nodes.\n",
      "WHITE played (6,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ○ ○ ●\n",
      " 6 | . . . . . ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 9, ○=WHITE 11, empty 44)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 2754 nodes.\n",
      "BLACK played (3,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● . .\n",
      " 5 | . . . . . ● ○ ●\n",
      " 6 | . . . . . ● ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 14, ○=WHITE 7, empty 43)\n",
      "Minimax(d=3) explored 13747 nodes.\n",
      "WHITE played (1,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ● ○ ● . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . . ● ○ ●\n",
      " 6 | . . . . . ● ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 11, ○=WHITE 11, empty 42)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 3364 nodes.\n",
      "BLACK played (4,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ● ○ ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . . ● ● ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 15, ○=WHITE 8, empty 41)\n",
      "Minimax(d=3) explored 14216 nodes.\n",
      "WHITE played (6,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ● ○ ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 13, ○=WHITE 11, empty 40)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 4144 nodes.\n",
      "BLACK played (0,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 8, empty 39)\n",
      "Minimax(d=3) explored 14623 nodes.\n",
      "WHITE played (2,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . . . . ● ● ●\n",
      " 6 | . . . . ○ ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 16, ○=WHITE 10, empty 38)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 4827 nodes.\n",
      "BLACK played (5,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 7, empty 37)\n",
      "Minimax(d=3) explored 15195 nodes.\n",
      "WHITE played (0,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● ○ ● . . . .\n",
      " 1 | . . ● ○ . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ○ ○ ○\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 9, empty 36)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 5351 nodes.\n",
      "BLACK played (7,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● ○ ● . . . .\n",
      " 1 | . . ● ○ . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 8, empty 35)\n",
      "Minimax(d=3) explored 15791 nodes.\n",
      "WHITE played (0,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . . ● ○ . . . .\n",
      " 2 | . . . ● ○ . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 10, empty 34)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 5621 nodes.\n",
      "BLACK played (1,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 8, empty 33)\n",
      "Minimax(d=3) explored 16074 nodes.\n",
      "WHITE played (0,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 10, empty 32)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 5710 nodes.\n",
      "BLACK played (3,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . ● . ● ● ● . .\n",
      " 4 | ○ ○ ● ● ● ● ● .\n",
      " 5 | . . . ● . ● ● ●\n",
      " 6 | . . . . ● ● ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 24, ○=WHITE 9, empty 31)\n",
      "Minimax(d=3) explored 16442 nodes.\n",
      "WHITE played (6,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● . . .\n",
      " 2 | . . . ○ ● . . .\n",
      " 3 | . ● . ○ ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 17, empty 30)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 6488 nodes.\n",
      "BLACK played (3,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● . . .\n",
      " 2 | . . . ● ● . . .\n",
      " 3 | . ● ● ● ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 15, empty 29)\n",
      "Minimax(d=3) explored 17633 nodes.\n",
      "WHITE played (0,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ● ○ ○ . . .\n",
      " 2 | . . . ○ ● . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 19, empty 28)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 7219 nodes.\n",
      "BLACK played (2,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ● ○ ○ . . .\n",
      " 2 | . . ● ● ● . . .\n",
      " 3 | . ● ● ● ● ● . .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . . . ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 17, empty 27)\n",
      "Minimax(d=3) explored 18539 nodes.\n",
      "WHITE played (5,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ○ ○ ○ . . .\n",
      " 2 | . . ○ ● ● . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● ● .\n",
      " 5 | . . ○ ○ . ● ● ●\n",
      " 6 | . . . ○ ○ ○ ○ ○\n",
      " 7 | . . . . ● ● ● ●\n",
      "    (●=BLACK 16, ○=WHITE 22, empty 26)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 8167 nodes.\n",
      "BLACK played (7,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ○ ○ ○ . . .\n",
      " 2 | . . ○ ● ● . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 18, empty 25)\n",
      "Minimax(d=3) explored 19241 nodes.\n",
      "WHITE played (1,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . . ○ ○ ○ ○ . .\n",
      " 2 | . . ○ ● ○ . . .\n",
      " 3 | . ● ○ ○ ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 21, empty 24)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 8763 nodes.\n",
      "BLACK played (1,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . ● ○ ○ ○ ○ . .\n",
      " 2 | . . ● ● ○ . . .\n",
      " 3 | . ● ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 19, empty 23)\n",
      "Minimax(d=3) explored 20121 nodes.\n",
      "WHITE played (2,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | . ○ ○ ○ ○ ○ . .\n",
      " 2 | . ○ ○ ○ ○ . . .\n",
      " 3 | . ○ ○ ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 24, empty 22)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 9169 nodes.\n",
      "BLACK played (1,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ● ○ ○ ○ ○ ○ . .\n",
      " 2 | . ● ○ ○ ○ . . .\n",
      " 3 | . ○ ● ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 22, empty 21)\n",
      "Minimax(d=3) explored 20737 nodes.\n",
      "WHITE played (2,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ . . .\n",
      " 3 | . ○ ● ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ● ● .\n",
      " 5 | . . ○ ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 25, empty 20)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 9511 nodes.\n",
      "BLACK played (3,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ . . .\n",
      " 3 | ● ● ● ● ● ● . .\n",
      " 4 | ○ ● ○ ● ● ● ● .\n",
      " 5 | . . ● ● . ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 22, empty 19)\n",
      "Minimax(d=3) explored 21086 nodes.\n",
      "WHITE played (5,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ . .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ . . .\n",
      " 3 | ● ● ○ ● ○ ● . .\n",
      " 4 | ○ ● ○ ○ ○ ● ● .\n",
      " 5 | . . ● ● ○ ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 27, empty 18)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 9717 nodes.\n",
      "BLACK played (0,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 1 | ○ ○ ○ ○ ○ ● . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | ● ● ○ ● ○ ● . .\n",
      " 4 | ○ ● ○ ○ ○ ● ● .\n",
      " 5 | . . ● ● ○ ● ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 25, empty 17)\n",
      "Minimax(d=3) explored 21580 nodes.\n",
      "WHITE played (2,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 1 | ○ ○ ○ ○ ○ ○ . .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ○ . .\n",
      " 4 | ○ ● ○ ○ ○ ○ ● .\n",
      " 5 | . . ● ● ○ ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 31, empty 16)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 9989 nodes.\n",
      "BLACK played (1,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 1 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 2 | ○ ○ ○ ○ ○ ● . .\n",
      " 3 | ● ● ○ ● ● ○ . .\n",
      " 4 | ○ ● ○ ● ○ ○ ● .\n",
      " 5 | . . ● ● ○ ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 28, empty 15)\n",
      "Minimax(d=3) explored 22142 nodes.\n",
      "WHITE played (0,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 2 | ○ ○ ○ ○ ○ ● . .\n",
      " 3 | ● ● ○ ● ● ○ . .\n",
      " 4 | ○ ● ○ ● ○ ○ ● .\n",
      " 5 | . . ● ● ○ ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 30, empty 14)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10192 nodes.\n",
      "BLACK played (3,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 2 | ○ ○ ○ ○ ○ ● . .\n",
      " 3 | ● ● ○ ● ● ● ● .\n",
      " 4 | ○ ● ○ ● ○ ● ● .\n",
      " 5 | . . ● ● ● ○ ● ●\n",
      " 6 | . . . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 24, ○=WHITE 27, empty 13)\n",
      "Minimax(d=3) explored 22423 nodes.\n",
      "WHITE played (6,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ● ○ ○ ○ ● ● .\n",
      " 5 | . . ○ ● ● ○ ● ●\n",
      " 6 | . ○ . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 33, empty 12)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10258 nodes.\n",
      "BLACK played (5,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ● ● ○ ○ ● ● .\n",
      " 5 | . ● ● ● ● ○ ● ●\n",
      " 6 | . ○ . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 31, empty 11)\n",
      "Minimax(d=3) explored 22618 nodes.\n",
      "WHITE played (5,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ○ ● ○ ○ ● ● .\n",
      " 5 | ○ ○ ○ ○ ○ ○ ● ●\n",
      " 6 | . ○ . ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 37, empty 10)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10363 nodes.\n",
      "BLACK played (6,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ● ● .\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 34, empty 9)\n",
      "Minimax(d=3) explored 22756 nodes.\n",
      "WHITE played (3,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ . .\n",
      " 3 | ● ● ○ ● ○ ○ ○ ○\n",
      " 4 | ○ ○ ● ○ ● ● ○ .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 38, empty 8)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10481 nodes.\n",
      "BLACK played (2,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● .\n",
      " 3 | ● ● ○ ● ○ ● ● ○\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 35, empty 7)\n",
      "Minimax(d=3) explored 22809 nodes.\n",
      "WHITE played (2,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 2 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 39, empty 6)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10531 nodes.\n",
      "BLACK played (1,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ● .\n",
      " 5 | ○ ○ ● ● ○ ○ ● ●\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 38, empty 5)\n",
      "Minimax(d=3) explored 22833 nodes.\n",
      "WHITE played (4,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 5 | ○ ○ ● ● ○ ○ ○ ○\n",
      " 6 | . ○ ● ● ● ○ ○ ○\n",
      " 7 | . . . ● ● ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 42, empty 4)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10557 nodes.\n",
      "BLACK played (7,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 5 | ○ ○ ● ● ○ ○ ○ ○\n",
      " 6 | . ● ● ● ● ○ ○ ○\n",
      " 7 | ● . . ● ● ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 41, empty 3)\n",
      "Minimax(d=3) explored 22848 nodes.\n",
      "WHITE played (6,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 5 | ○ ○ ● ● ○ ○ ○ ○\n",
      " 6 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 7 | ● . . ● ● ● ● ●\n",
      "    (●=BLACK 16, ○=WHITE 46, empty 2)\n",
      "ExpectimaxRisky(d=3, eps=0.10) explored 10562 nodes.\n",
      "BLACK played (7,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ● ● ○ ● ○ ○ ○\n",
      " 5 | ○ ● ● ● ○ ○ ○ ○\n",
      " 6 | ○ ● ● ○ ○ ○ ○ ○\n",
      " 7 | ● ● . ● ● ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 42, empty 1)\n",
      "Minimax(d=3) explored 22849 nodes.\n",
      "WHITE played (7,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ● ○ ○ ● ○ ○ ○\n",
      " 5 | ○ ● ○ ● ○ ○ ○ ○\n",
      " 6 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 7 | ● ● ○ ● ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 47, empty 0)\n",
      "Final:\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ○ ○ ○ ○ ○ ○ ○ ●\n",
      " 2 | ○ ○ ○ ○ ○ ○ ● ○\n",
      " 3 | ● ● ○ ● ○ ● ○ ○\n",
      " 4 | ○ ● ○ ○ ● ○ ○ ○\n",
      " 5 | ○ ● ○ ● ○ ○ ○ ○\n",
      " 6 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 7 | ● ● ○ ● ● ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 47, empty 0)\n",
      "Winner: WHITE\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . . . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 2, ○=WHITE 2, empty 60)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 72 nodes.\n",
      "BLACK played (2,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 1, empty 59)\n",
      "Minimax(d=3) explored 22927 nodes.\n",
      "WHITE played (4,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 3, empty 58)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 220 nodes.\n",
      "BLACK played (5,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 5, ○=WHITE 2, empty 57)\n",
      "Minimax(d=3) explored 23090 nodes.\n",
      "WHITE played (1,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 5, empty 56)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 424 nodes.\n",
      "BLACK played (4,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 3, empty 55)\n",
      "Minimax(d=3) explored 23293 nodes.\n",
      "WHITE played (6,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 6, empty 54)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 670 nodes.\n",
      "BLACK played (5,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● . . .\n",
      " 5 | . . . . ● ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 5, empty 53)\n",
      "Minimax(d=3) explored 23525 nodes.\n",
      "WHITE played (4,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ○ . . .\n",
      " 4 | . ● ● ● ● ○ . .\n",
      " 5 | . . . . ● ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 5, ○=WHITE 7, empty 52)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 1243 nodes.\n",
      "BLACK played (7,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ○ . . .\n",
      " 4 | . ● ● ● ● ○ . .\n",
      " 5 | . . . . ● ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 5, empty 51)\n",
      "Minimax(d=3) explored 24020 nodes.\n",
      "WHITE played (4,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ○ . . .\n",
      " 4 | ○ ○ ○ ○ ○ ○ . .\n",
      " 5 | . . . . ● ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 4, ○=WHITE 10, empty 50)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 1499 nodes.\n",
      "BLACK played (2,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ● ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 7, ○=WHITE 8, empty 49)\n",
      "Minimax(d=3) explored 24483 nodes.\n",
      "WHITE played (6,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . .\n",
      " 6 | . . . . . ○ ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 5, ○=WHITE 11, empty 48)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 2110 nodes.\n",
      "BLACK played (0,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . ● ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . .\n",
      " 6 | . . . . . ○ ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 9, empty 47)\n",
      "Minimax(d=3) explored 24797 nodes.\n",
      "WHITE played (6,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . ● ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . .\n",
      " 6 | . . . . . ○ ○ ○\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 7, ○=WHITE 11, empty 46)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 2609 nodes.\n",
      "BLACK played (5,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . ● ● . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . ●\n",
      " 6 | . . . . . ○ ○ ●\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 9, ○=WHITE 10, empty 45)\n",
      "Minimax(d=3) explored 25043 nodes.\n",
      "WHITE played (1,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . ●\n",
      " 6 | . . . . . ○ ○ ●\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 12, empty 44)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 3361 nodes.\n",
      "BLACK played (6,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ● ○ . ●\n",
      " 6 | . . . . ● ● ● ●\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 12, ○=WHITE 9, empty 43)\n",
      "Minimax(d=3) explored 26324 nodes.\n",
      "WHITE played (7,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . . . ○ ●\n",
      "    (●=BLACK 10, ○=WHITE 12, empty 42)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 4274 nodes.\n",
      "BLACK played (7,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . . ● ● ●\n",
      "    (●=BLACK 12, ○=WHITE 11, empty 41)\n",
      "Minimax(d=3) explored 27004 nodes.\n",
      "WHITE played (7,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ● ○ . .\n",
      " 5 | . . . . ○ ○ . ●\n",
      " 6 | . . . . ○ ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 11, ○=WHITE 13, empty 40)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 5208 nodes.\n",
      "BLACK played (3,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ● ● . .\n",
      " 5 | . . . . ○ ● . ●\n",
      " 6 | . . . . ○ ● ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 16, ○=WHITE 9, empty 39)\n",
      "Minimax(d=3) explored 27795 nodes.\n",
      "WHITE played (4,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ○ ○ ○ ○ .\n",
      " 5 | . . . . ○ ○ . ●\n",
      " 6 | . . . . ○ ● ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 13, ○=WHITE 13, empty 38)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 6490 nodes.\n",
      "BLACK played (5,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ . . . .\n",
      " 3 | . . . ● ● ● . .\n",
      " 4 | ○ ○ ○ ● ● ○ ○ .\n",
      " 5 | . . . ● ○ ○ . ●\n",
      " 6 | . . . . ● ● ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 10, empty 37)\n",
      "Minimax(d=3) explored 29062 nodes.\n",
      "WHITE played (2,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ ○ . . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ○ ● ○ ○ ○ .\n",
      " 5 | . . . ● ○ ○ . ●\n",
      " 6 | . . . . ● ● ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 13, ○=WHITE 15, empty 36)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 7866 nodes.\n",
      "BLACK played (2,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ● ○ ● ○ .\n",
      " 5 | . . . ● ○ ● . ●\n",
      " 6 | . . . . ● ● ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 9, empty 35)\n",
      "Minimax(d=3) explored 30812 nodes.\n",
      "WHITE played (5,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . ○ ● . . . .\n",
      " 2 | . . ● ○ ● ● . .\n",
      " 3 | . . . ○ ○ ● . .\n",
      " 4 | ○ ○ ○ ● ○ ○ ○ .\n",
      " 5 | . . . ● ○ ○ ○ ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 15, ○=WHITE 15, empty 34)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 9452 nodes.\n",
      "BLACK played (0,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . . ● ● . . . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ○ ● ● . .\n",
      " 4 | ○ ○ ○ ● ○ ● ○ .\n",
      " 5 | . . . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 10, empty 33)\n",
      "Minimax(d=3) explored 32192 nodes.\n",
      "WHITE played (0,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● ○ ● . . . .\n",
      " 1 | . . ● ○ . . . .\n",
      " 2 | . . ● ● ○ ● . .\n",
      " 3 | . . . ○ ● ○ . .\n",
      " 4 | ○ ○ ○ ● ○ ● ○ .\n",
      " 5 | . . . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 18, ○=WHITE 14, empty 32)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 10893 nodes.\n",
      "BLACK played (1,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ○ ● ○ . .\n",
      " 4 | ○ ○ ○ ● ○ ● ○ .\n",
      " 5 | . . . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 12, empty 31)\n",
      "Minimax(d=3) explored 33421 nodes.\n",
      "WHITE played (0,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ○ ● ○ . .\n",
      " 4 | ○ ○ ○ ● ○ ● ○ .\n",
      " 5 | . . . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 14, empty 30)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 11763 nodes.\n",
      "BLACK played (5,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . . ● ● ● . . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ● ● ○ . .\n",
      " 4 | ○ ○ ● ● ○ ● ○ .\n",
      " 5 | . ● . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 12, empty 29)\n",
      "Minimax(d=3) explored 34485 nodes.\n",
      "WHITE played (0,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ○ . . .\n",
      " 2 | . . ● ● ○ ● . .\n",
      " 3 | . . . ● ○ ○ . .\n",
      " 4 | ○ ○ ● ● ○ ● ○ .\n",
      " 5 | . ● . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 17, empty 28)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 12989 nodes.\n",
      "BLACK played (1,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● ● . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ● ○ ○ . .\n",
      " 4 | ○ ○ ● ● ○ ● ○ .\n",
      " 5 | . ● . ● ○ ○ ● ●\n",
      " 6 | . . . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 15, empty 27)\n",
      "Minimax(d=3) explored 36150 nodes.\n",
      "WHITE played (6,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● ● . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ● ○ ○ . .\n",
      " 4 | ○ ○ ● ● ○ ● ○ .\n",
      " 5 | . ○ . ● ○ ○ ● ●\n",
      " 6 | . ○ . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 21, ○=WHITE 17, empty 26)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 13816 nodes.\n",
      "BLACK played (6,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● ● . .\n",
      " 2 | . . ● ● ● ● . .\n",
      " 3 | . . . ● ○ ○ . .\n",
      " 4 | ○ ○ ● ● ○ ● ○ .\n",
      " 5 | . ● . ● ○ ○ ● ●\n",
      " 6 | ● ○ . . ● ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 16, empty 25)\n",
      "Minimax(d=3) explored 37845 nodes.\n",
      "WHITE played (6,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● ● . .\n",
      " 2 | . . ● ○ ● ● . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ● . ○ ○ ○ ● ●\n",
      " 6 | ● ○ . ○ ○ ○ ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 17, ○=WHITE 23, empty 24)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 14986 nodes.\n",
      "BLACK played (6,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● ● . .\n",
      " 2 | . . ● ○ ● ● . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ● . ○ ○ ○ ● ●\n",
      " 6 | ● ● ● ● ● ● ● ●\n",
      " 7 | . . . . ○ ● ● ●\n",
      "    (●=BLACK 22, ○=WHITE 19, empty 23)\n",
      "Minimax(d=3) explored 39464 nodes.\n",
      "WHITE played (7,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● ● . .\n",
      " 2 | . . ● ○ ● ● . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ . ○ ○ ○ ● ●\n",
      " 6 | ● ○ ○ ● ● ● ● ●\n",
      " 7 | . ○ . . ○ ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 23, empty 22)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 15566 nodes.\n",
      "BLACK played (5,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ● ● . .\n",
      " 2 | . . ● ○ ● ● . .\n",
      " 3 | . . . ○ ● ○ . .\n",
      " 4 | ○ ○ ● ● ○ ● ○ .\n",
      " 5 | . ○ ● ● ● ● ● ●\n",
      " 6 | ● ○ ○ ● ● ● ● ●\n",
      " 7 | . ○ . . ○ ● ● ●\n",
      "    (●=BLACK 25, ○=WHITE 18, empty 21)\n",
      "Minimax(d=3) explored 40703 nodes.\n",
      "WHITE played (1,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ○ ○ ○ ○ .\n",
      " 2 | . . ● ○ ● ○ . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ○ ● ● ● ● ●\n",
      " 6 | ● ○ ○ ● ● ● ● ●\n",
      " 7 | . ○ . . ○ ● ● ●\n",
      "    (●=BLACK 19, ○=WHITE 25, empty 20)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 16638 nodes.\n",
      "BLACK played (1,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . . ● ● ● ● ● ●\n",
      " 2 | . . ● ○ ● ○ . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ○ ● ● ● ● ●\n",
      " 6 | ● ○ ○ ● ● ● ● ●\n",
      " 7 | . ○ . . ○ ● ● ●\n",
      "    (●=BLACK 24, ○=WHITE 21, empty 19)\n",
      "Minimax(d=3) explored 41772 nodes.\n",
      "WHITE played (0,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . . ● ● ● ● ○ ●\n",
      " 2 | . . ● ○ ● ○ . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ○ ● ● ● ● ●\n",
      " 6 | ● ○ ○ ● ● ● ● ●\n",
      " 7 | . ○ . . ○ ● ● ●\n",
      "    (●=BLACK 23, ○=WHITE 23, empty 18)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 17597 nodes.\n",
      "BLACK played (7,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . . ● ● ● ● ○ ●\n",
      " 2 | . . ● ○ ● ○ . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ● ● ● ● ● ●\n",
      " 6 | ● ○ ● ● ● ● ● ●\n",
      " 7 | . ○ ● . ○ ● ● ●\n",
      "    (●=BLACK 26, ○=WHITE 21, empty 17)\n",
      "Minimax(d=3) explored 42680 nodes.\n",
      "WHITE played (7,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . . ● ● ● ● ○ ●\n",
      " 2 | . . ● ○ ● ○ . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ● ○ ● ○ ● ●\n",
      " 6 | ● ○ ○ ○ ○ ● ● ●\n",
      " 7 | . ○ ○ ○ ○ ● ● ●\n",
      "    (●=BLACK 20, ○=WHITE 28, empty 16)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 18217 nodes.\n",
      "BLACK played (7,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . . ● ● ● ● ○ ●\n",
      " 2 | . . ● ○ ● ○ . .\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ● ○ ● ○ ● ●\n",
      " 6 | ● ● ○ ○ ○ ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 26, ○=WHITE 23, empty 15)\n",
      "Minimax(d=3) explored 43406 nodes.\n",
      "WHITE played (2,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . . ● ● ● ● ○ ○\n",
      " 2 | . . ● ○ ● ○ . ○\n",
      " 3 | . . . ○ ○ ○ . .\n",
      " 4 | ○ ○ ● ○ ○ ● ○ .\n",
      " 5 | . ○ ● ○ ● ○ ● ●\n",
      " 6 | ● ● ○ ○ ○ ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 25, ○=WHITE 25, empty 14)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 18799 nodes.\n",
      "BLACK played (3,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . . ● ● ● ● ○ ○\n",
      " 2 | . . ● ○ ● ○ . ○\n",
      " 3 | . . . ○ ○ ○ . ●\n",
      " 4 | ○ ○ ● ○ ○ ● ● .\n",
      " 5 | . ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 29, ○=WHITE 22, empty 13)\n",
      "Minimax(d=3) explored 43917 nodes.\n",
      "WHITE played (1,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ . ○\n",
      " 3 | . . . ○ ○ ○ . ●\n",
      " 4 | ○ ○ ● ○ ○ ● ● .\n",
      " 5 | . ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 24, ○=WHITE 28, empty 12)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19092 nodes.\n",
      "BLACK played (2,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ● ● ○\n",
      " 3 | . . . ○ ○ ● . ●\n",
      " 4 | ○ ○ ● ○ ● ● ● .\n",
      " 5 | . ○ ● ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 30, ○=WHITE 23, empty 11)\n",
      "Minimax(d=3) explored 43968 nodes.\n",
      "WHITE played (3,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ ○ ○\n",
      " 3 | . . . ○ ○ ○ ○ ●\n",
      " 4 | ○ ○ ● ○ ● ○ ● .\n",
      " 5 | . ○ ● ● ○ ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 25, ○=WHITE 29, empty 10)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19210 nodes.\n",
      "BLACK played (3,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ ○ ○\n",
      " 3 | . . ● ● ● ● ● ●\n",
      " 4 | ○ ○ ● ● ● ○ ● .\n",
      " 5 | . ○ ● ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 32, ○=WHITE 23, empty 9)\n",
      "Minimax(d=3) explored 43989 nodes.\n",
      "WHITE played (4,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ ○ ○\n",
      " 3 | . . ● ● ● ● ○ ○\n",
      " 4 | ○ ○ ● ● ● ○ ○ ○\n",
      " 5 | . ○ ● ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 29, ○=WHITE 27, empty 8)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19299 nodes.\n",
      "BLACK played (5,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ ○ ○\n",
      " 3 | . . ● ● ● ● ○ ○\n",
      " 4 | ○ ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ● ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 32, ○=WHITE 25, empty 7)\n",
      "Minimax(d=3) explored 44017 nodes.\n",
      "WHITE played (3,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ ○ ○\n",
      " 3 | ○ . ● ● ● ● ○ ○\n",
      " 4 | ○ ○ ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 30, ○=WHITE 28, empty 6)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19342 nodes.\n",
      "BLACK played (3,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | . . ○ ○ ● ○ ○ ○\n",
      " 3 | ○ ● ● ● ● ● ○ ○\n",
      " 4 | ○ ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 32, ○=WHITE 27, empty 5)\n",
      "WHITE passes.\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19361 nodes.\n",
      "BLACK played (2,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | ● . ○ ○ ● ○ ○ ○\n",
      " 3 | ● ● ● ● ● ● ○ ○\n",
      " 4 | ● ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 35, ○=WHITE 25, empty 4)\n",
      "WHITE passes.\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19372 nodes.\n",
      "BLACK played (2,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . ○\n",
      " 1 | . ○ ○ ○ ○ ○ ○ ○\n",
      " 2 | ● ● ● ● ● ○ ○ ○\n",
      " 3 | ● ● ● ● ● ● ○ ○\n",
      " 4 | ● ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 38, ○=WHITE 23, empty 3)\n",
      "WHITE passes.\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19377 nodes.\n",
      "BLACK played (0,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ● . ○\n",
      " 1 | . ○ ○ ○ ● ● ○ ○\n",
      " 2 | ● ● ● ● ● ● ○ ○\n",
      " 3 | ● ● ● ● ● ● ○ ○\n",
      " 4 | ● ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 42, ○=WHITE 20, empty 2)\n",
      "Minimax(d=3) explored 44019 nodes.\n",
      "WHITE played (0,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | . ○ ○ ○ ● ● ○ ○\n",
      " 2 | ● ● ● ● ● ● ○ ○\n",
      " 3 | ● ● ● ● ● ● ○ ○\n",
      " 4 | ● ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 41, ○=WHITE 22, empty 1)\n",
      "ExpectimaxRisky(d=3, eps=0.30) explored 19378 nodes.\n",
      "BLACK played (1,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ● ● ● ● ● ● ○ ○\n",
      " 2 | ● ● ● ● ● ● ○ ○\n",
      " 3 | ● ● ● ● ● ● ○ ○\n",
      " 4 | ● ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 45, ○=WHITE 19, empty 0)\n",
      "Final:\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 1 | ● ● ● ● ● ● ○ ○\n",
      " 2 | ● ● ● ● ● ● ○ ○\n",
      " 3 | ● ● ● ● ● ● ○ ○\n",
      " 4 | ● ● ● ● ● ○ ○ ○\n",
      " 5 | ● ● ○ ● ● ● ● ●\n",
      " 6 | ● ● ● ○ ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 45, ○=WHITE 19, empty 0)\n",
      "Winner: BLACK\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . . . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 2, ○=WHITE 2, empty 60)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 72 nodes.\n",
      "BLACK played (2,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . . ● ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 1, empty 59)\n",
      "Minimax(d=3) explored 44097 nodes.\n",
      "WHITE played (4,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ○ . . .\n",
      " 5 | . . . . . . . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 3, empty 58)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 220 nodes.\n",
      "BLACK played (5,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . . . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 5, ○=WHITE 2, empty 57)\n",
      "Minimax(d=3) explored 44260 nodes.\n",
      "WHITE played (1,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . . ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 3, ○=WHITE 5, empty 56)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 424 nodes.\n",
      "BLACK played (4,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . . .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 6, ○=WHITE 3, empty 55)\n",
      "Minimax(d=3) explored 44463 nodes.\n",
      "WHITE played (6,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . . . . . .\n",
      " 1 | . . . ○ . . . .\n",
      " 2 | . . . ○ . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 4, ○=WHITE 6, empty 54)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 670 nodes.\n",
      "BLACK played (0,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . . ● . . . .\n",
      " 3 | . . . ● ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 8, ○=WHITE 3, empty 53)\n",
      "Minimax(d=3) explored 44559 nodes.\n",
      "WHITE played (2,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . . ○ ● . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 7, ○=WHITE 5, empty 52)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 927 nodes.\n",
      "BLACK played (2,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . . . ● . . . .\n",
      " 2 | . ● ● ● . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 9, ○=WHITE 4, empty 51)\n",
      "Minimax(d=3) explored 44794 nodes.\n",
      "WHITE played (1,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . . . ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ○ ● . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 8, ○=WHITE 6, empty 50)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 1277 nodes.\n",
      "BLACK played (0,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | . ● . ● . . . .\n",
      " 1 | . ● . ● . . . .\n",
      " 2 | . ● ○ ● . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 10, ○=WHITE 5, empty 49)\n",
      "Minimax(d=3) explored 45201 nodes.\n",
      "WHITE played (0,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ● . ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ○ ● . . . .\n",
      " 3 | . . . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 9, ○=WHITE 7, empty 48)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 1542 nodes.\n",
      "BLACK played (3,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ● . ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ● ● . . . .\n",
      " 3 | . ● . ○ ● . . .\n",
      " 4 | . ● ● ● ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 11, ○=WHITE 6, empty 47)\n",
      "Minimax(d=3) explored 45458 nodes.\n",
      "WHITE played (4,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ● . ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ● ● . . . .\n",
      " 3 | . ● . ○ ● . . .\n",
      " 4 | ○ ○ ○ ○ ○ . . .\n",
      " 5 | . . . . . ○ . .\n",
      " 6 | . . . . . . ○ .\n",
      " 7 | . . . . . . . .\n",
      "    (●=BLACK 8, ○=WHITE 10, empty 46)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 1870 nodes.\n",
      "BLACK played (7,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ● . ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ● ● . . . .\n",
      " 3 | . ● . ● ● . . .\n",
      " 4 | ○ ○ ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 13, ○=WHITE 6, empty 45)\n",
      "Minimax(d=3) explored 45679 nodes.\n",
      "WHITE played (0,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ● ● . . . .\n",
      " 3 | . ● . ● ● . . .\n",
      " 4 | ○ ○ ○ ○ ● . . .\n",
      " 5 | . . . . . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 12, ○=WHITE 8, empty 44)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 1953 nodes.\n",
      "BLACK played (5,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ● . . . .\n",
      " 1 | . ○ . ● . . . .\n",
      " 2 | . ● ● ● . . . .\n",
      " 3 | . ● . ● ● . . .\n",
      " 4 | ○ ○ ○ ● ● . . .\n",
      " 5 | . . ● . . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 14, ○=WHITE 7, empty 43)\n",
      "Minimax(d=3) explored 45903 nodes.\n",
      "WHITE played (0,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | . ● ○ ● . . . .\n",
      " 3 | . ○ . ● ● . . .\n",
      " 4 | ○ ○ ○ ● ● . . .\n",
      " 5 | . . ● . . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 10, ○=WHITE 12, empty 42)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 2085 nodes.\n",
      "BLACK played (5,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | . ● ○ ● . . . .\n",
      " 3 | . ● . ● ● . . .\n",
      " 4 | ○ ● ● ● ● . . .\n",
      " 5 | . ● ● . . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 14, ○=WHITE 9, empty 41)\n",
      "Minimax(d=3) explored 46125 nodes.\n",
      "WHITE played (2,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ● . . . .\n",
      " 3 | . ● . ● ● . . .\n",
      " 4 | ○ ● ● ● ● . . .\n",
      " 5 | . ● ● . . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 13, ○=WHITE 11, empty 40)\n",
      "BLACK passes.\n",
      "Minimax(d=3) explored 46301 nodes.\n",
      "WHITE played (5,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ . . . .\n",
      " 3 | . ○ . ○ ● . . .\n",
      " 4 | ○ ● ○ ○ ● . . .\n",
      " 5 | . ● ● ○ . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 17, empty 39)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 2375 nodes.\n",
      "BLACK played (2,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | . ○ . ● ● . . .\n",
      " 4 | ○ ● ● ○ ● . . .\n",
      " 5 | . ● ● ○ . ● . .\n",
      " 6 | . . . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 11, ○=WHITE 15, empty 38)\n",
      "Minimax(d=3) explored 46689 nodes.\n",
      "WHITE played (6,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | . ○ . ● ● . . .\n",
      " 4 | ○ ○ ● ○ ● . . .\n",
      " 5 | . ○ ○ ○ . ● . .\n",
      " 6 | . ○ . . . . ● .\n",
      " 7 | . . . . . . . ●\n",
      "    (●=BLACK 8, ○=WHITE 19, empty 37)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 2739 nodes.\n",
      "BLACK played (7,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | . ○ . ● ● . . .\n",
      " 4 | ○ ○ ● ● ● . . .\n",
      " 5 | . ○ ● ○ . ● . .\n",
      " 6 | . ● . . . . ● .\n",
      " 7 | ● . . . . . . ●\n",
      "    (●=BLACK 12, ○=WHITE 16, empty 36)\n",
      "Minimax(d=3) explored 47104 nodes.\n",
      "WHITE played (7,1)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | . ○ . ● ● . . .\n",
      " 4 | ○ ○ ● ● ● . . .\n",
      " 5 | . ○ ● ○ . ● . .\n",
      " 6 | . ○ . . . . ● .\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 11, ○=WHITE 18, empty 35)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 3283 nodes.\n",
      "BLACK played (6,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | . ○ . ● ● . . .\n",
      " 4 | ○ ○ ● ● ● . . .\n",
      " 5 | . ● ● ○ . ● . .\n",
      " 6 | ● ○ . . . . ● .\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 13, ○=WHITE 17, empty 34)\n",
      "Minimax(d=3) explored 47491 nodes.\n",
      "WHITE played (4,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | . ○ . ● ○ . . .\n",
      " 4 | ○ ○ ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ● . .\n",
      " 6 | ● ○ . . . . ● .\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 9, ○=WHITE 22, empty 33)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 3990 nodes.\n",
      "BLACK played (3,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | ● ○ . ● ○ . . .\n",
      " 4 | ○ ● ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ● . .\n",
      " 6 | ● ○ . . . . ● .\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 11, ○=WHITE 21, empty 32)\n",
      "Minimax(d=3) explored 48131 nodes.\n",
      "WHITE played (6,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | . ○ . ○ . . . .\n",
      " 2 | ○ ○ ○ ○ ● . . .\n",
      " 3 | ● ○ . ● ○ . . .\n",
      " 4 | ○ ● ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ○ . .\n",
      " 6 | ● ○ . . . ○ ● .\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 10, ○=WHITE 23, empty 31)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 5255 nodes.\n",
      "BLACK played (1,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ○ . ○ . . . .\n",
      " 2 | ● ○ ○ ○ ● . . .\n",
      " 3 | ● ○ . ● ○ . . .\n",
      " 4 | ○ ● ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ○ . .\n",
      " 6 | ● ○ . . . ○ ● .\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 12, ○=WHITE 22, empty 30)\n",
      "Minimax(d=3) explored 48793 nodes.\n",
      "WHITE played (6,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ○ . ○ . . . .\n",
      " 2 | ● ○ ○ ○ ● . . .\n",
      " 3 | ● ○ . ● ○ . . .\n",
      " 4 | ○ ● ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ○ . .\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 11, ○=WHITE 24, empty 29)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 6369 nodes.\n",
      "BLACK played (1,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ . ● ○ . . .\n",
      " 4 | ○ ● ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ○ . .\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 14, ○=WHITE 22, empty 28)\n",
      "Minimax(d=3) explored 49362 nodes.\n",
      "WHITE played (3,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ○ . . .\n",
      " 4 | ○ ● ○ ○ ○ ○ . .\n",
      " 5 | . ● ● ○ . ○ . .\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 13, ○=WHITE 24, empty 27)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 7334 nodes.\n",
      "BLACK played (5,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ● ○ ● . . .\n",
      " 4 | ○ ● ○ ● ● ○ . .\n",
      " 5 | . ● ● ● ● ○ . .\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 19, ○=WHITE 19, empty 26)\n",
      "Minimax(d=3) explored 49952 nodes.\n",
      "WHITE played (5,0)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ○ ○ ● ● ○ . .\n",
      " 5 | ○ ○ ○ ○ ○ ○ . .\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . . . ●\n",
      "    (●=BLACK 13, ○=WHITE 26, empty 25)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 7966 nodes.\n",
      "BLACK played (7,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ○ ○ ● ● ○ . .\n",
      " 5 | ○ ○ ○ ○ ● ○ . .\n",
      " 6 | ● ○ . . . ● ○ ○\n",
      " 7 | ● ○ . . . . ● ●\n",
      "    (●=BLACK 16, ○=WHITE 24, empty 24)\n",
      "Minimax(d=3) explored 50419 nodes.\n",
      "WHITE played (7,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ○ ○ ● ● ○ . .\n",
      " 5 | ○ ○ ○ ○ ● ○ . .\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . ○ ● ●\n",
      "    (●=BLACK 15, ○=WHITE 26, empty 23)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 8574 nodes.\n",
      "BLACK played (5,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ○ ○ ● ● ● . .\n",
      " 5 | ○ ○ ○ ○ ● ● ● .\n",
      " 6 | ● ○ . . . ○ ● ○\n",
      " 7 | ● ○ . . . ○ ● ●\n",
      "    (●=BLACK 19, ○=WHITE 23, empty 22)\n",
      "Minimax(d=3) explored 50854 nodes.\n",
      "WHITE played (5,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ○ ○ ● ● ● . .\n",
      " 5 | ○ ○ ○ ○ ○ ○ ○ ○\n",
      " 6 | ● ○ . . . ○ ○ ○\n",
      " 7 | ● ○ . . . ○ ● ●\n",
      "    (●=BLACK 15, ○=WHITE 28, empty 21)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 9099 nodes.\n",
      "BLACK played (6,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ● ○ . . . .\n",
      " 2 | ● ● ● ○ ● . . .\n",
      " 3 | ● ○ ● ○ ● . . .\n",
      " 4 | ○ ○ ● ● ● ● . .\n",
      " 5 | ○ ○ ● ● ○ ○ ○ ○\n",
      " 6 | ● ● ● . . ○ ○ ○\n",
      " 7 | ● ○ . . . ○ ● ●\n",
      "    (●=BLACK 22, ○=WHITE 22, empty 20)\n",
      "Minimax(d=3) explored 51509 nodes.\n",
      "WHITE played (7,2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ○ ○ ● ● ● . .\n",
      " 5 | ○ ○ ○ ● ○ ○ ○ ○\n",
      " 6 | ● ○ ○ . . ○ ○ ○\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 15, ○=WHITE 30, empty 19)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 9552 nodes.\n",
      "BLACK played (6,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ● . . .\n",
      " 3 | ● ○ ○ ○ ● . . .\n",
      " 4 | ○ ● ○ ● ● ● . .\n",
      " 5 | ○ ○ ● ● ● ○ ○ ○\n",
      " 6 | ● ● ● ● . ○ ○ ○\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 21, ○=WHITE 25, empty 18)\n",
      "Minimax(d=3) explored 51814 nodes.\n",
      "WHITE played (3,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . . .\n",
      " 3 | ● ○ ○ ○ ○ ○ . .\n",
      " 4 | ○ ● ○ ● ○ ○ . .\n",
      " 5 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 6 | ● ● ○ ● . ○ ○ ○\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 15, ○=WHITE 32, empty 17)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 9845 nodes.\n",
      "BLACK played (3,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . . .\n",
      " 3 | ● ● ● ● ● ● ● .\n",
      " 4 | ○ ● ○ ● ○ ● . .\n",
      " 5 | ○ ○ ● ○ ● ○ ○ ○\n",
      " 6 | ● ● ○ ● . ○ ○ ○\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 22, ○=WHITE 26, empty 16)\n",
      "Minimax(d=3) explored 52128 nodes.\n",
      "WHITE played (2,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . . ○\n",
      " 3 | ● ● ● ● ● ● ○ .\n",
      " 4 | ○ ● ○ ● ○ ○ . .\n",
      " 5 | ○ ○ ● ○ ○ ○ ○ ○\n",
      " 6 | ● ● ○ ○ . ○ ○ ○\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 18, ○=WHITE 31, empty 15)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10214 nodes.\n",
      "BLACK played (4,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . . ○\n",
      " 3 | ● ● ● ● ● ● ○ .\n",
      " 4 | ○ ● ○ ● ○ ○ . ●\n",
      " 5 | ○ ○ ● ○ ○ ○ ○ ●\n",
      " 6 | ● ● ○ ○ . ○ ○ ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 21, ○=WHITE 29, empty 14)\n",
      "Minimax(d=3) explored 52221 nodes.\n",
      "WHITE played (4,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . . ○\n",
      " 3 | ● ● ● ● ● ○ ○ .\n",
      " 4 | ○ ● ○ ● ○ ○ ○ ●\n",
      " 5 | ○ ○ ● ○ ○ ○ ○ ●\n",
      " 6 | ● ● ○ ○ . ○ ○ ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 20, ○=WHITE 31, empty 13)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10384 nodes.\n",
      "BLACK played (3,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . . ○\n",
      " 3 | ● ● ● ● ● ● ● ●\n",
      " 4 | ○ ● ○ ● ○ ○ ○ ●\n",
      " 5 | ○ ○ ● ○ ○ ○ ○ ●\n",
      " 6 | ● ● ○ ○ . ○ ○ ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 23, ○=WHITE 29, empty 12)\n",
      "Minimax(d=3) explored 52281 nodes.\n",
      "WHITE played (2,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . . .\n",
      " 2 | ● ● ○ ○ ○ . ○ ○\n",
      " 3 | ● ● ● ● ● ○ ○ ●\n",
      " 4 | ○ ● ○ ● ○ ○ ○ ●\n",
      " 5 | ○ ○ ● ○ ○ ○ ○ ●\n",
      " 6 | ● ● ○ ○ . ○ ○ ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 21, ○=WHITE 32, empty 11)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10535 nodes.\n",
      "BLACK played (1,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . ● .\n",
      " 2 | ● ● ○ ○ ○ . ● ○\n",
      " 3 | ● ● ● ● ● ○ ● ●\n",
      " 4 | ○ ● ○ ● ○ ○ ● ●\n",
      " 5 | ○ ○ ● ○ ○ ○ ● ●\n",
      " 6 | ● ● ○ ○ . ○ ● ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 27, ○=WHITE 27, empty 10)\n",
      "Minimax(d=3) explored 52381 nodes.\n",
      "WHITE played (1,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . ● ○\n",
      " 2 | ● ● ○ ○ ○ . ○ ○\n",
      " 3 | ● ● ● ● ● ○ ● ●\n",
      " 4 | ○ ● ○ ● ○ ○ ● ●\n",
      " 5 | ○ ○ ● ○ ○ ○ ● ●\n",
      " 6 | ● ● ○ ○ . ○ ● ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 26, ○=WHITE 29, empty 9)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10729 nodes.\n",
      "BLACK played (6,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . . .\n",
      " 1 | ● ● ○ ○ . . ● ○\n",
      " 2 | ● ● ○ ○ ○ . ○ ○\n",
      " 3 | ● ● ● ● ● ○ ● ●\n",
      " 4 | ○ ● ● ● ● ○ ● ●\n",
      " 5 | ○ ○ ● ● ● ● ● ●\n",
      " 6 | ● ● ● ● ● ● ● ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 35, ○=WHITE 21, empty 8)\n",
      "Minimax(d=3) explored 52505 nodes.\n",
      "WHITE played (0,6)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . ○ .\n",
      " 1 | ● ● ○ ○ . . ○ ○\n",
      " 2 | ● ● ○ ○ ○ . ○ ○\n",
      " 3 | ● ● ● ● ● ○ ● ●\n",
      " 4 | ○ ● ● ● ● ○ ● ●\n",
      " 5 | ○ ○ ● ● ● ● ● ●\n",
      " 6 | ● ● ● ● ● ● ● ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 34, ○=WHITE 23, empty 7)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10816 nodes.\n",
      "BLACK played (0,7)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . ○ ●\n",
      " 1 | ● ● ○ ○ . . ○ ●\n",
      " 2 | ● ● ○ ○ ○ . ○ ●\n",
      " 3 | ● ● ● ● ● ○ ● ●\n",
      " 4 | ○ ● ● ● ● ○ ● ●\n",
      " 5 | ○ ○ ● ● ● ● ● ●\n",
      " 6 | ● ● ● ● ● ● ● ●\n",
      " 7 | ● ○ ○ . . ○ ● ●\n",
      "    (●=BLACK 37, ○=WHITE 21, empty 6)\n",
      "Minimax(d=3) explored 52538 nodes.\n",
      "WHITE played (7,3)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . ○ ●\n",
      " 1 | ● ● ○ ○ . . ○ ●\n",
      " 2 | ● ● ○ ○ ○ . ○ ●\n",
      " 3 | ● ● ● ○ ● ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ○ ● ● ● ●\n",
      " 7 | ● ○ ○ ○ . ○ ● ●\n",
      "    (●=BLACK 32, ○=WHITE 27, empty 5)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10861 nodes.\n",
      "BLACK played (7,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . ○ ●\n",
      " 1 | ● ● ○ ○ . . ○ ●\n",
      " 2 | ● ● ○ ○ ○ . ○ ●\n",
      " 3 | ● ● ● ○ ● ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ● ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 38, ○=WHITE 22, empty 4)\n",
      "Minimax(d=3) explored 52546 nodes.\n",
      "WHITE played (2,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ . ○ ●\n",
      " 1 | ● ● ○ ○ . . ○ ●\n",
      " 2 | ● ● ○ ○ ○ ○ ○ ●\n",
      " 3 | ● ● ● ○ ○ ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ● ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 37, ○=WHITE 24, empty 3)\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10873 nodes.\n",
      "BLACK played (0,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ● ● ●\n",
      " 1 | ● ● ○ ○ . . ● ●\n",
      " 2 | ● ● ○ ○ ○ ○ ○ ●\n",
      " 3 | ● ● ● ○ ○ ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ● ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 40, ○=WHITE 22, empty 2)\n",
      "WHITE passes.\n",
      "ExpectimaxRisky(d=3, eps=0.50) explored 10877 nodes.\n",
      "BLACK played (1,4)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ● ● ●\n",
      " 1 | ● ● ● ● ● . ● ●\n",
      " 2 | ● ● ○ ● ● ● ○ ●\n",
      " 3 | ● ● ● ○ ● ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ● ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 47, ○=WHITE 16, empty 1)\n",
      "Minimax(d=3) explored 52547 nodes.\n",
      "WHITE played (1,5)\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ● ● ●\n",
      " 1 | ● ● ● ● ● ○ ● ●\n",
      " 2 | ● ● ○ ● ○ ○ ○ ●\n",
      " 3 | ● ● ● ○ ● ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ● ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 45, ○=WHITE 19, empty 0)\n",
      "Final:\n",
      "    0 1 2 3 4 5 6 7\n",
      "   ----------------\n",
      " 0 | ○ ○ ○ ○ ○ ● ● ●\n",
      " 1 | ● ● ● ● ● ○ ● ●\n",
      " 2 | ● ● ○ ● ○ ○ ○ ●\n",
      " 3 | ● ● ● ○ ● ○ ● ●\n",
      " 4 | ○ ● ● ○ ● ○ ● ●\n",
      " 5 | ○ ○ ● ○ ● ● ● ●\n",
      " 6 | ● ● ○ ● ● ● ● ●\n",
      " 7 | ● ● ● ● ● ● ● ●\n",
      "    (●=BLACK 45, ○=WHITE 19, empty 0)\n",
      "Winner: BLACK\n",
      "Expectimax (10% error) vs Minimax: -1 | nodes: 10562\n",
      "Expectimax (30% error) vs Minimax: 1 | nodes: 19378\n",
      "Expectimax (50% error) vs Minimax: 1 | nodes: 10877\n"
     ]
    }
   ],
   "source": [
    "minimax_agent = MinimaxAgent(depth=3)\n",
    "ab_agent = AlphaBetaAgent(depth=3)\n",
    "\n",
    "res, _ = play_game(minimax_agent, ab_agent, seed=10, verbose=True)\n",
    "print(f\"Minimax vs AlphaBeta (Depth 3): {res} | nodes: {minimax_agent.nodes} / {ab_agent.nodes}\")\n",
    "\n",
    "expectimax_10 = ExpectimaxRiskyAgent(depth=3, blunder_rate=0.1)\n",
    "expectimax_30 = ExpectimaxRiskyAgent(depth=3, blunder_rate=0.3)\n",
    "expectimax_50 = ExpectimaxRiskyAgent(depth=3, blunder_rate=0.5)\n",
    "\n",
    "res_10, _ = play_game(expectimax_10, minimax_agent, seed=11, verbose=True)\n",
    "res_30, _ = play_game(expectimax_30, minimax_agent, seed=12, verbose=True)\n",
    "res_50, _ = play_game(expectimax_50, minimax_agent, seed=13, verbose=True)\n",
    "\n",
    "print(f\"Expectimax (10% error) vs Minimax: {res_10} | nodes: {expectimax_10.nodes}\")\n",
    "print(f\"Expectimax (30% error) vs Minimax: {res_30} | nodes: {expectimax_30.nodes}\")\n",
    "print(f\"Expectimax (50% error) vs Minimax: {res_50} | nodes: {expectimax_50.nodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad5716",
   "metadata": {},
   "source": [
    "## Human vs Agent\n",
    "play with your ai\n",
    "\n",
    "You need to install and enable ipywidgets for the GUI to work.\n",
    "\n",
    "**On Mac/Linux**: Usually works after `pip install ipywidgets`\n",
    "\n",
    "**On Windows**: You may need to install and enable the extension:\n",
    "```python\n",
    "! pip install ipywidgets\n",
    "! jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "! jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "```\n",
    "\n",
    "Run the cell below to check your setup and install if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ac76cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ipywidgets installation...\n",
      "✓ ipywidgets is already installed\n",
      "\n",
      "Enabling Jupyter widget extensions for Windows...\n",
      "Note: Extension enable commands may need manual execution\n",
      "Run: jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
      "\n",
      "✓ Setup complete! You may need to refresh your browser if widgets don't appear.\n",
      "On Windows: If the GUI still doesn't show, try restarting Jupyter notebook.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_and_setup_widgets():\n",
    "    \"\"\"Install and enable ipywidgets for Jupyter\"\"\"\n",
    "    print(\"Checking ipywidgets installation...\")\n",
    "    \n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        print(\"✓ ipywidgets is already installed\")\n",
    "    except ImportError:\n",
    "        print(\"Installing ipywidgets...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ipywidgets\", \"-q\"])\n",
    "        print(\"✓ ipywidgets installed\")\n",
    "    \n",
    "    import platform\n",
    "    if platform.system() == \"Windows\":\n",
    "        print(\"\\nEnabling Jupyter widget extensions for Windows...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\", \"--sys-prefix\"], \n",
    "                                 stderr=subprocess.DEVNULL)\n",
    "            print(\"✓ Extension enabled\")\n",
    "        except:\n",
    "            print(\"Note: Extension enable commands may need manual execution\")\n",
    "            print(\"Run: jupyter nbextension enable --py widgetsnbextension --sys-prefix\")\n",
    "    \n",
    "    print(\"\\n✓ Setup complete! You may need to refresh your browser if widgets don't appear.\")\n",
    "    print(\"On Windows: If the GUI still doesn't show, try restarting Jupyter notebook.\")\n",
    "\n",
    "install_and_setup_widgets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bceaf46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e3337ad29940aa95e55aa8c904a208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>🎮 Othello: Human vs AI</h2>'), Dropdown(description='AI Agent:', index=5, optio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output, HTML\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(\"ERROR: ipywidgets is not installed or not working.\")\n",
    "    print(\"Please run the setup cell above to install it.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    print(\"\\nIf you're on Windows, you may need to run these commands in terminal:\")\n",
    "    print(\"  pip install ipywidgets jupyter nbextension enable --py widgetsnbextension --sys-prefix\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "if not WIDGETS_AVAILABLE:\n",
    "    raise ImportError(\"Cannot start GUI without ipywidgets\")\n",
    "\n",
    "class OthelloGUI:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.agent = None\n",
    "        self.human_color = None\n",
    "        self.game_active = False\n",
    "        self.output = widgets.Output()\n",
    "        self.last_ai_move = None\n",
    "        \n",
    "        self.cell_size = 60  \n",
    "        self.setup_menu()\n",
    "    \n",
    "    def setup_menu(self):\n",
    "        \"\"\"Setup the agent selection menu\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        title = widgets.HTML(\"<h2>🎮 Othello: Human vs AI</h2>\")\n",
    "        \n",
    "        agent_dropdown = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Random', 'random'),\n",
    "                ('Greedy Flips', 'greedy'),\n",
    "                ('Minimax (depth=3)', 'minimax_3'),\n",
    "                ('Minimax (depth=4)', 'minimax_4'),\n",
    "                ('Alpha-Beta (depth=3)', 'alphabeta_3'),\n",
    "                ('Alpha-Beta (depth=4)', 'alphabeta_4'),\n",
    "                ('Alpha-Beta (depth=5)', 'alphabeta_5'),\n",
    "                ('Expectimax (depth=3, ε=0.2)', 'expectimax_3'),\n",
    "                ('Expectimax (depth=4, ε=0.2)', 'expectimax_4'),\n",
    "            ],\n",
    "            value='alphabeta_4',\n",
    "            description='AI Agent:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        color_dropdown = widgets.Dropdown(\n",
    "            options=[('Black (●) - Go First', BLACK), ('White (○) - Go Second', WHITE)],\n",
    "            value=BLACK,\n",
    "            description='Your Color:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        start_button = widgets.Button(\n",
    "            description='Start Game',\n",
    "            button_style='success',\n",
    "            icon='play'\n",
    "        )\n",
    "        \n",
    "        def on_start_clicked(b):\n",
    "            agent_type = agent_dropdown.value\n",
    "            if agent_type == 'random':\n",
    "                self.agent = RandomAgent()\n",
    "            elif agent_type == 'greedy':\n",
    "                self.agent = GreedyFlipAgent()\n",
    "            elif agent_type == 'minimax_3':\n",
    "                self.agent = MinimaxAgent(depth=3)\n",
    "            elif agent_type == 'minimax_4':\n",
    "                self.agent = MinimaxAgent(depth=4)\n",
    "            elif agent_type == 'alphabeta_3':\n",
    "                self.agent = AlphaBetaAgent(depth=3)\n",
    "            elif agent_type == 'alphabeta_4':\n",
    "                self.agent = AlphaBetaAgent(depth=4)\n",
    "            elif agent_type == 'alphabeta_5':\n",
    "                self.agent = AlphaBetaAgent(depth=5)\n",
    "            elif agent_type == 'expectimax_3':\n",
    "                self.agent = ExpectimaxRiskyAgent(depth=3, blunder_rate=0.2)\n",
    "            elif agent_type == 'expectimax_4':\n",
    "                self.agent = ExpectimaxRiskyAgent(depth=4, blunder_rate=0.2)\n",
    "            \n",
    "            self.human_color = color_dropdown.value\n",
    "            self.start_game()\n",
    "        \n",
    "        start_button.on_click(on_start_clicked)\n",
    "        \n",
    "        menu_box = widgets.VBox([\n",
    "            title,\n",
    "            agent_dropdown,\n",
    "            color_dropdown,\n",
    "            start_button\n",
    "        ])\n",
    "        \n",
    "        display(menu_box)\n",
    "    \n",
    "    def start_game(self):\n",
    "        \"\"\"Start a new game\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        board = initial_board()\n",
    "        self.state = OthelloState.from_board(board, BLACK)\n",
    "        self.game_active = True\n",
    "        self.last_ai_move = None\n",
    "        \n",
    "        self.create_game_ui()\n",
    "        \n",
    "        if self.state.player != self.human_color:\n",
    "            self.make_ai_move()\n",
    "    \n",
    "    def create_game_ui(self):\n",
    "        \"\"\"Create the game board UI\"\"\"\n",
    "        self.info_label = widgets.HTML()\n",
    "        \n",
    "        self.buttons = []\n",
    "        rows = []\n",
    "        \n",
    "        display(HTML(\"\"\"\n",
    "        <style>\n",
    "        .othello-cell button {\n",
    "            border-radius: 50% !important;\n",
    "            font-size: 24px !important;\n",
    "            font-weight: bold !important;\n",
    "            color: white !important;\n",
    "            padding: 0 !important;\n",
    "            transition: all 0.2s ease;\n",
    "            box-shadow: inset 0 2px 4px rgba(0,0,0,0.2);\n",
    "        }\n",
    "        .othello-cell button:hover:not(:disabled) {\n",
    "            transform: scale(1.15);\n",
    "            box-shadow: 0 6px 12px rgba(0,0,0,0.4), inset 0 2px 4px rgba(0,0,0,0.2);\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\"))\n",
    "        \n",
    "        for r in range(N):\n",
    "            row = []\n",
    "            for c in range(N):\n",
    "                btn = widgets.Button(\n",
    "                    description='',\n",
    "                    layout=widgets.Layout(\n",
    "                        width=f'{self.cell_size}px', \n",
    "                        height=f'{self.cell_size}px',\n",
    "                        margin='2px'\n",
    "                    )\n",
    "                )\n",
    "                btn.add_class('othello-cell')\n",
    "                btn.on_click(lambda b, row=r, col=c: self.on_cell_click(row, col))\n",
    "                row.append(btn)\n",
    "            rows.append(widgets.HBox(row, layout=widgets.Layout(margin='0')))\n",
    "            self.buttons.append(row)\n",
    "        \n",
    "        board_box = widgets.VBox(\n",
    "            rows, \n",
    "            layout=widgets.Layout(\n",
    "                margin='10px 0px',\n",
    "                padding='15px',\n",
    "                background='#1b5e20',\n",
    "                border_radius='10px'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.pass_button = widgets.Button(\n",
    "            description='Pass',\n",
    "            button_style='warning',\n",
    "            icon='forward',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        self.pass_button.on_click(lambda b: self.on_pass())\n",
    "        \n",
    "        new_game_button = widgets.Button(\n",
    "            description='New Game',\n",
    "            button_style='info',\n",
    "            icon='refresh',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        new_game_button.on_click(lambda b: self.setup_menu())\n",
    "        \n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        controls = widgets.HBox([self.pass_button, new_game_button])\n",
    "        game_ui = widgets.VBox([\n",
    "            self.info_label,\n",
    "            board_box,\n",
    "            controls,\n",
    "            self.status_output\n",
    "        ])\n",
    "        \n",
    "        display(game_ui)\n",
    "        self.update_display()\n",
    "    \n",
    "    def update_display(self):\n",
    "        \"\"\"Update the board display\"\"\"\n",
    "        board = self.state.to_board()\n",
    "        legal = legal_moves(board, self.state.player) if self.game_active else []\n",
    "        \n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                btn = self.buttons[r][c]\n",
    "                cell = board[r][c]\n",
    "                \n",
    "                btn.description = ''\n",
    "                btn.disabled = True\n",
    "                \n",
    "                is_last_move = self.last_ai_move and (r, c) == self.last_ai_move\n",
    "                \n",
    "                if cell == BLACK:\n",
    "                    if is_last_move:\n",
    "                        btn.style.button_color = '#000000'\n",
    "                        btn.layout.border = '3px solid #FFA500'\n",
    "                    else:\n",
    "                        btn.style.button_color = '#000000'\n",
    "                        btn.layout.border = '2px solid #888'\n",
    "                    btn.disabled = True\n",
    "                    \n",
    "                elif cell == WHITE:\n",
    "                    if is_last_move:\n",
    "                        btn.style.button_color = '#F5F5DC'\n",
    "                        btn.layout.border = '3px solid #FFA500'\n",
    "                    else:\n",
    "                        btn.style.button_color = '#F5F5DC'\n",
    "                        btn.layout.border = '2px solid #888'\n",
    "                    btn.disabled = True\n",
    "                    \n",
    "                elif self.game_active and self.state.player == self.human_color and (r, c) in legal:\n",
    "                    btn.description = '+'\n",
    "                    btn.style.button_color = '#28a745'\n",
    "                    btn.layout.border = '2px solid #1e7e34'\n",
    "                    btn.disabled = False\n",
    "                    \n",
    "                else:\n",
    "                    btn.style.button_color = '#2d6a4f'\n",
    "                    btn.layout.border = '2px solid #1b4332'\n",
    "                    btn.disabled = True\n",
    "        \n",
    "        b, w, e = count_discs(board)\n",
    "        info_html = f\"\"\"\n",
    "        <div style='font-size: 18px; margin-bottom: 10px; padding: 10px; background: #f0f0f0; border-radius: 5px;'>\n",
    "            <b>⚫ Black:</b> {b} | <b>⚪ White:</b> {w} | <b>Empty:</b> {e}<br>\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.game_active:\n",
    "            current_player = \"⚫ Black\" if self.state.player == BLACK else \"⚪ White\"\n",
    "            if self.state.player == self.human_color:\n",
    "                info_html += f\"<b style='color: green;'>🎯 Your turn ({current_player})</b><br>\"\n",
    "                if self.last_ai_move:\n",
    "                    info_html += f\"<small style='color: #ff8800;'>🔶 Orange = AI's last move at ({self.last_ai_move[0]}, {self.last_ai_move[1]})</small>\"\n",
    "            else:\n",
    "                info_html += f\"<b style='color: blue;'>🤖 AI's turn ({current_player})</b>\"\n",
    "        else:\n",
    "            if b > w:\n",
    "                winner = \"⚫ Black\"\n",
    "            elif w > b:\n",
    "                winner = \"⚪ White\"\n",
    "            else:\n",
    "                winner = \"🤝 Draw\"\n",
    "            info_html += f\"<b style='color: red;'>🏁 Game Over! Winner: {winner}</b>\"\n",
    "        \n",
    "        info_html += \"</div>\"\n",
    "        self.info_label.value = info_html\n",
    "        \n",
    "        if self.game_active and self.state.player == self.human_color and legal == [None]:\n",
    "            self.pass_button.disabled = False\n",
    "        else:\n",
    "            self.pass_button.disabled = True\n",
    "    \n",
    "    def on_cell_click(self, row, col):\n",
    "        \"\"\"Handle cell click\"\"\"\n",
    "        if not self.game_active or self.state.player != self.human_color:\n",
    "            return\n",
    "        \n",
    "        board = self.state.to_board()\n",
    "        legal = legal_moves(board, self.state.player)\n",
    "        \n",
    "        if (row, col) not in legal:\n",
    "            return\n",
    "        \n",
    "        new_board = apply_move(board, (row, col), self.state.player)\n",
    "        self.state = OthelloState.from_board(new_board, opponent(self.state.player))\n",
    "        \n",
    "        self.last_ai_move = None\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"You played ({row}, {col})\")\n",
    "        \n",
    "        self.update_display()\n",
    "        \n",
    "        if is_terminal(self.state):\n",
    "            self.end_game()\n",
    "            return\n",
    "        \n",
    "        if self.state.player != self.human_color:\n",
    "            self.make_ai_move()\n",
    "    \n",
    "    def on_pass(self):\n",
    "        \"\"\"Handle pass button\"\"\"\n",
    "        if not self.game_active or self.state.player != self.human_color:\n",
    "            return\n",
    "        \n",
    "        board = self.state.to_board()\n",
    "        legal = legal_moves(board, self.state.player)\n",
    "        \n",
    "        if legal != [None]:\n",
    "            return\n",
    "        \n",
    "        self.state = OthelloState.from_board(board, opponent(self.state.player))\n",
    "        \n",
    "        self.last_ai_move = None\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"You passed.\")\n",
    "        \n",
    "        self.update_display()\n",
    "        \n",
    "        if is_terminal(self.state):\n",
    "            self.end_game()\n",
    "            return\n",
    "        \n",
    "        if self.state.player != self.human_color:\n",
    "            self.make_ai_move()\n",
    "    \n",
    "    def make_ai_move(self):\n",
    "        \"\"\"Make AI move\"\"\"\n",
    "        import time\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        board = self.state.to_board()\n",
    "        legal = legal_moves(board, self.state.player)\n",
    "        \n",
    "        if legal == [None]:\n",
    "            self.state = OthelloState.from_board(board, opponent(self.state.player))\n",
    "            self.last_ai_move = None\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"AI ({self.agent.name}) passed.\")\n",
    "            self.update_display()\n",
    "            \n",
    "            if is_terminal(self.state):\n",
    "                self.end_game()\n",
    "            return\n",
    "        \n",
    "        move = self.agent.action(self.state)\n",
    "        \n",
    "        if move is not None:\n",
    "            new_board = apply_move(board, move, self.state.player)\n",
    "            self.state = OthelloState.from_board(new_board, opponent(self.state.player))\n",
    "            \n",
    "            self.last_ai_move = move\n",
    "            \n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"AI ({self.agent.name}) played ({move[0]}, {move[1]})\")\n",
    "        \n",
    "        self.update_display()\n",
    "        \n",
    "        if is_terminal(self.state):\n",
    "            self.end_game()\n",
    "    \n",
    "    def end_game(self):\n",
    "        \"\"\"End the game\"\"\"\n",
    "        self.game_active = False\n",
    "        self.update_display()\n",
    "        \n",
    "        b, w, e = count_discs(self.state.to_board())\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            if b > w:\n",
    "                winner = \"Black\"\n",
    "                if self.human_color == BLACK:\n",
    "                    print(f\"🎉 You won! Black: {b}, White: {w}\")\n",
    "                else:\n",
    "                    print(f\"😞 AI won! Black: {b}, White: {w}\")\n",
    "            elif w > b:\n",
    "                winner = \"White\"\n",
    "                if self.human_color == WHITE:\n",
    "                    print(f\"🎉 You won! White: {w}, Black: {b}\")\n",
    "                else:\n",
    "                    print(f\"😞 AI won! White: {w}, Black: {b}\")\n",
    "            else:\n",
    "                print(f\"🤝 Draw! Black: {b}, White: {w}\")\n",
    "\n",
    "gui = OthelloGUI()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
